% !TEX root = ../FundationsDataScience.tex

\chapter{Denoising}


Together with compression, denoising is the most important processing application, that is pervasive in almost any signal or image processing pipeline. Indeed, data acquisition always comes with some kind of noise, so modeling this noise and removing it efficiently is crucial.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Noise Modeling}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Noise in Images}

Image acquisition devices always produce some noise. Figure \ref{fig-noise-examples} shows images produced by different hardware, where the regularity of the underlying signal and the statistics of the noise is very different.

\myfigure{
\tabtrois{
\image{denoising}{.3}{noise-camera}&
\image{denoising}{.3}{noise-confocal}&
\image{denoising}{.3}{noise-sar}\\
Digital camera & Confocal imaging & SAR imaging
}
}{%
	Example of noise in different imaging device. %	
}{fig-noise-examples}

One should thus model both the acquisition process and the statistics of the noise to fit the imaging process. Then one should also model the regularity and geometry of the clean signal to choose a basis adapted to its representation. 
This chapter describes how thresholding methods can be used to perform denoising in some specific situations where the noise statistics are close to being Gaussian and the mixing operator is a sum or can be approximated by a sum. 

Since noise perturbs discrete measurements acquired by some hardware, in the following, we consider only finite dimensional signal $f \in \CC^N$.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Image Formation}
\label{subsec-image-formation}

Figure \ref{fig-additive-noise-model} shows an idealized view of the image formation process, that mixes a clean image $f_0$ with a noise $w$ to obtain noisy observations $f = f_0 \oplus w$, where $\oplus$ might for instance be a sum or a multiplication.

\myfigure{
\image{denoising}{.8}{additive-noise-model}
}{%
	Image formation with noise modeling and denoising pipepline. %	
}{fig-additive-noise-model}

Statistical modeling considers $w$ as a random vector with known distribution, while numerical computation are usually done on a single realization of this random vector, still denoted as $w$.

%%
\paragraph{Additive Noise.}

The simplest model for such image formation consists in assuming that it is an additive perturbation of a clean signal $f_0$
\eq{
	f = f_0 + w
}
where $w$ is the noise residual. Statistical noise modeling assume that $w$ is a random vector, and in practice one only observes a realization of this vector. This modeling thus implies that the image $f$ to be processed is also a random vector. Figure \ref{fig-noise-example-1d} and \ref{fig-noise-example-2d} show examples of noise addition to a clean signal and a clean image.

\myfigure{
\tabdeux{
\image{denoising}{.45}{noise-example-1d-clean}&
\image{denoising}{.45}{noise-example-1d-noisy}\\
$f_0$ & $f$
}
}{%
	1-D additive noise example.%	
}{fig-noise-example-1d}


\myfigure{
\tabdeux{
\image{denoising}{.3}{noise-example-2d-clean}&
\image{denoising}{.3}{noise-example-2d-noisy}\\
$f_0$ & $f$
}
}{%
	2-D additive noise example.%	
}{fig-noise-example-2d}

The simplest noise model assumes that each entry $w_n$ of the noise is a Gaussian random variable of variance $\si^2$, and that the $w_n$ are independent, i.e. $w \sim \Nn(0,\Id_N)$. This is the white noise model.

Depending on the image acquisition device, one should consider different noise distributions, such as for instance uniform noise $w_n \in [-a,a]$ or Impulse noise
\eq{
	\PP(w_n=x) \propto e^{-|x/\si|^\al}
	\qwhereq \al<2
}

In many situations, the noise perturbation is not additive, and for instance its intensity might depend on the intensity of the signal. This is the case with Poisson and multiplicative noises considered in Section \ref{sect-data-dept-noises}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Denoiser}

A denoiser (also called estimator) is an estimation $\tilde f$ of $f_0$ computed from the observation $f$ alone. It is thus also a random vector that depends on the noise $w$. Since $f$ is a random vector of mean $f_0$, the numerical denoising process corresponds to the estimation of the mean of a random vector from a single realization. Figure \ref{fig-denoising-clean} shows an example of denoising.

The quality of a denoiser is measured using the average mean square risk $\EE_w(\norm{f_0-\tilde f}^2)$.
where $\EE_w$ is the esperance (averaging) with respect to the noise $w$. Since $f_0$ is unknown, this corresponds to a theoretical measure of performance, that is bounded using a mathematical analysis. In the numerical experiments, one observes a single realization $f^{r} \sim f_0 + w$, and the performance is estimated from this single denoising using the SNR
\eq{
	\text{SNR}(\tilde f^r,f0) = -20\log_{10}( \norm{\tilde f^r-f_0}/\norm{f_0} ),
}
where $\tilde f^r$ should be computed from the single realization $f^r$ of $f$.
%
In the following, with an abuse of notation, when displaying single realization, we ignore the exponent $^r$.
%
The SNR is expressed in ``decibels'', denoted dB. This measure of performance requires the knowledge of the clean signal $f_0$, and should thus only be considered as an experimentation tool, that might not be available in a real life denoising scenario where clean data are not available. Furthermore, the use of an $\ell^2$ measure of performance is questionable, and one should also observe the result to judge of the visual quality of the denoising.


\myfigure{
\tabtrois{
\image{denoising}{.32}{denoising-clean}&
\image{denoising}{.32}{denoising-noisy}&
\image{denoising}{.32}{denoising-denoised}\\
$f_0$ & $f$ & $\tilde f$
}
}{%
	Left: clean image, center: noisy image, right: denoised image.%	
}{fig-denoising-clean}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear Denoising using Filtering}
\label{sec-linear-denoising}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Translation Invariant Estimators}

A linear estimator $\Ee(f) = \tilde f$ of $f_0$ depends linearly on $f$, so that $\Ee(f+g) = \Ee(f)+\Ee(g)$.
A translation invariant estimator commutes with translation, so that $\Ee(f_\tau) = \Ee(f)_\tau$, where $f_\tau(t) = f(t-\tau)$.
Such a denoiser can always be written as a filtering
\eq{
	\tilde f = f \star h
}
where $h \in \RR^N$ is a (low pass) filter, that should satisfy at least 
\eq{
	\sum_n h_n = \hat h_0 = 1
}
where $\hat h$ is the discrete Fourier transform. 

Figure \ref{fig-filtering-denoised} shows an example of denoising using a low pass filter.

\myfigure{
\tabdeux{
\image{denoising}{.45}{filtering-noisy}&
\image{denoising}{.45}{filtering-noisy-fourier}\\
$f$ & $\log(\hat f)$ \\
\image{denoising}{.45}{filtering-filter}&
\image{denoising}{.45}{filtering-filter-fourier}\\
$f$ & $\log(\hat h)$ \\
\image{denoising}{.45}{filtering-denoised}&
\image{denoising}{.45}{filtering-denoised-fourier}\\
$f \star h$ & $\log(\hat f \cdot \hat h) = \log(\hat h) + \log(\hat h)$ 
}
}{%
	Denoising by filtering over the spacial (left) and Fourier (right) domains.%	
}{fig-filtering-denoised}

The filtering strength is usually controlled the width $s$ of $h$. A typical example is the Gaussian filter
\eql{\label{eq-gaussian-filters}
	\foralls -N/2 < i \leq N/2, \quad h_{s,i} = \frac{1}{Z_s} \exp\pa{ -\frac{ i^2 }{ 2s^2} }
}
where $Z_s$ ensures that $\sum_i h_{s,i}=1$ (low pass). Figure \ref{fig-filtering-denoised} shows the effect of Gaussian filtering over the spacial and Fourier domains.

\if 0
\myfigure{
\image{denoising}{.3}{blurring-1d-noisy}\\
\image{denoising}{.3}{blurring-1d-denoised}
}{%
	Noisy signal (top) and denoised signal using filtering (bottom). %	
}{fig-blurring-1d}
\myfigure{
\image{denoising}{.3}{blurring-2d-noisy}
\image{denoising}{.3}{blurring-2d-denoised}
}{%
	Noisy image (top) and denoised image using filtering (bottom). %	
}{fig-blurring-2d}
\fi 

Figure \ref{fig-filtering-progression} shows the effect of low pass filtering on a signal and an image with an increasing filter width $s$.  Linear filtering introduces a blur and are thus only efficient to denoise smooth signals and image. For signals and images with discontinuities, this blur deteriorates the signal. Removing a large amount of noise necessitates to also smooth significantly edges and singularities.

\myfigure{
\image{denoising}{.7}{filtering-progression-1d}
\image{denoising}{.2}{filtering-progression-2d}
}{%
	Denoising using a filter of increasing width $s$.%	
}{fig-filtering-progression}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Optimal Filter Selection}
\label{subsec-optimal-selection-filter}

The selection of an optimal filter is a difficult task. Its choice depends both on the regularity of the (unknown) data $f_0$ and the noise level $\si$. A simpler option is to optimize the filter width $s$ among a parametric family of filters, such as for instance the Gaussian filters defined in \eqref{eq-gaussian-filters}.

The denoising error can be decomposed as
\eq{
	\norm{\tilde f - f_0} \leq \norm{ h_s \star f_0 - f_0 } + \norm{ h_s \star w }
}
The filter width $s$ should be optimized to perform a tradeoff between removing enough noise ($\norm{h_s \star w}$ decreases with $s$) and not smoothing too much the singularities ($(\norm{h_s \star f_0 - f_0}$ increases with $s$).

Figure \eqref{fig-iltering-optimal-curve} shows the oracle SNR performance, defined in \eqref{eq-defn-snf}.

\myfigure{
\image{denoising}{.48}{filtering-optimal-1d-curve}
\image{denoising}{.46}{filtering-optimal-2d-curve}
}{%
	Curves of SNR as a function of the filtering width in 1-D (left) and 2-D (right).%	
}{fig-iltering-optimal-curve}

Figure \ref{fig-filtering-optimal-1d} and \ref{fig-filtering-optimal-2d} show the results of denoising using the optimal filter width $s^\star$ that minimizes the SNR for a given noisy observation. 

\myfigure{
\image{denoising}{.48}{filtering-optimal-1d-noisy}
\image{denoising}{.48}{filtering-optimal-1d-denoised}
}{%
	Noisy image (left) and denoising (right) using the optimal filter width.%	
}{fig-filtering-optimal-1d}

\myfigure{
\image{denoising}{.3}{filtering-optimal-2d-noisy}
\image{denoising}{.3}{filtering-optimal-2d-denoised}
}{%
	Noisy image (left) and denoising (right) using the optimal filter width.%	
}{fig-filtering-optimal-2d}

These optimal filtering appear quite noisy, and the optimal SNR choice is usually quite conservative. Increasing the filter width introduces a strong blurring that deteriorates the SNR, although it might look visually more pleasant.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Wiener Filter}

If one has a random model both for the noise $w \sim W$ and for for the signal $f_0 \sim F$, one can derives an optimal filters in average over both the noise and the signal realizations. One further assumes that $w$ and $f_0$ are independent realization.
The optimal $h$ thus minimizes
\eq{
	\EE_{W,F}(\norm{h \star (F+W)-F}^2)
}
If both $F$ is wide-sense stationary, and $W$ is a Gaussian white noise of variance $\si^2$, then the optimal filer is known as the Wiener filter
\eq{
	\hat h_\om = \frac{|\hat F_\om|^2}{|\hat F_\om|^2+\si^2}
}
where $|\hat F|^2$  is the power spectrum of $F$, 
\eq{
	\hat F_\om = \hat C_\om \qwhereq
	C_n = \EE( \dotp{F}{F[\cdot+n]} ),
}
the Fourier transform of an infinite vector is defined in Section~\ref{sec-dft}.

In practice, one rarely has such a random model for the signal, and interesting signals are often not stationary. Most signals exhibit discontinuities, and are thus poorly restored with filtering.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Denoising and Linear Approximation}

In order to study linear (and also non-linear, see the section bellow) denoising without assuming a random signal model, one should use approximation theory as studied in Chapter~\ref{chap-approx}. 
%
We thus consider an ortho-basis $\Bb = (\psi_m)_m$ of $\RR^N$, and consider a simple denoising obtained by keeping only the $M$ first term elements of the approximation of the noisy observation in $\Bb$
\eql{\label{eq-ideal-linear-denoise}
	\tilde f \eqdef \sum_{m=1}^M \dotp{f}{\psi_m} \psi_m.
}
This is a linear projection on the space spanned by $(\psi_m)_{m=1}^M$.
%
This denoising scheme is thus parameterized by some integer $M>0$, increasing $M$ increases the denoising strength. 
%
For instance, when $\Bb$ is the discrete Fourier basis, this corresponds to an ideal low-pass filter against a (discretized) Dirichlet kernel.

More advanced linear denoising operator can be designed by computing weighted average $\sum_{m} \la_m \dotp{f}{\psi_m} \psi_m$, and~\eqref{eq-ideal-linear-denoise} is retrieve when using binary weights $\al_n=1$ for $n \leq M$, and $\al_n=0$ otherwise. The asymptotic theoretical performances described by the following theorem are however not improved by using non-binary weights.

\begin{thm}\label{thm-linear-denoising}
	We assume that $f_0 \in \RR^N$ has a linear approximation error decay that satisfies 
	\eq{
		\foralls M, \quad 
		\norm{f_0 - f_{0,M}^{\text{lin}} }^2 \leq C M^{-2\be}
		\qwhereq
		f_{0,M}^{\text{lin}} \eqdef \sum_{m=1}^M \dotp{f_0}{\psi_m} \psi_m
	} 
	for some constant $C$. Then the linear denoising error using~\eqref{eq-ideal-linear-denoise} satisfies
	\eq{
		\EE(\norm{f_0 - \tilde f}^2) \leq 2 C^{\frac{1}{2\be+1}} \si^{2-\frac{1}{\be+1/2}}, 
	}
	when choosing 
	\eql{\label{eq-lineardenoising-scale}
		M = C^{\frac{1}{2\be+1}} \si^{-\frac{2}{2\be+1}}. 
	}
\end{thm}

\begin{proof}
	One has, thanks to the ortho-normality of $(\psi_m)_m$
	\begin{align*}
		\EE(\norm{f_0 - \tilde f}^2) &= 
		\EE( \sum_m \dotp{f_0 - \tilde f}{\psi_m}^2 )
		= 
		\EE( \sum_{m=1}^M \dotp{f_0 - f}{\psi_m}^2 + \sum_{m>M} \dotp{f_0 }{\psi_m}^2 ) \\
		&=
		\EE\pa{\sum_{m=1}^M \dotp{w}{\psi_m}^2} + \sum_{m>M} \dotp{f_0}{\psi_m}^2
		= M \si^2 + \norm{f_0 - f_{0,M}^{\text{lin}}}^2 \\
		&\leq M \si^2 + C M^{-2\be}.
	\end{align*}
	Here we use the fundamental fact that $(\dotp{w}{\psi_m})_m$ is also $\Nn(0,\si^2\Id_N)$.
	% 
	Choosing $M$ such that $M \si^2 = C M^{-2\be}$, i.e. $M = C^{\frac{1}{2\be+1}} \si^{-\frac{2}{2\be+1}}$ leads 
	to 
	\eq{
		\EE(\norm{f_0 - \tilde f}^2)  = 2 C M^{-2\be} = 2 C C^{-\frac{2\be}{2\be+1}} \si^{\frac{4\be}{2\be+1}}
		= 2 C^{\frac{1}{2\be+1}} \si^{2 - \frac{1}{\be+1/2}}.
	}
\end{proof}

There are several important remark regarding this simple but important result:
\begin{rs}
	\item Thanks to the decay of the linear approximation error, the denoising error $\EE(\norm{f_0 - \tilde f}^2)$ is bounded \textit{independently} of the sampling size $N$, although the input noise level $\EE(\norm{w}^2)=N\si^2$ growth with $N$.
	\item If the signal is well approximated linearly, i.e. if $\be$ is large, then the denoising error decays fast when the noise level $\si$ drops to zero. The upper bound approaches the optimal rate $\si^2$ by taking $\be$ large enough. 
	\item This theory is finite dimensional, i.e. this computation makes only sense when introducing some discretization step $N$. This is natural because random noise vectors of finite energy are necessarily finite dimensional. For the choice~\eqref{eq-lineardenoising-scale} to be realizable, one should however have $M \leq N$, i.e. $N \geq C^{\frac{1}{2\be+1}} \si^{-\frac{2}{2\be+1}}$. Thus $N$ should increase when the noise diminish for the denoising effect to kick-in. 
	\item Section~\ref{sec-linear-approx-error} bounds the linear approximation error for infinite dimensional signal and image model. This theory can be applied provided that the discretization error is smaller than the denoising error, i.e. once again, one should use $N$ large enough. 
\end{rs}

A typical setup where this denoising theorem can be applied is for the Sobolev signal and image model detailed in Section~\ref{subsec-smooth-class}.  In the discrete setting, where the sampling size $N$ is intended to grow (specially if $\si$ diminishes), one can similarly consider a ``Sobolev-like'' model, and similarely as for Proposition~\ref{prop-sobol-fourier-lin}, this model implies a decay of the linear approximation error.

\begin{prop}
	Assuming that 
	\eql{\label{eq-discr-sobol}
		\sum_{m=1}^N m^{2\al} |\dotp{f_0}{\psi_m}|^{2} \leq C
	}
	then 
	\eq{
		\foralls M, \quad 
		\norm{f_0 - f_{0,M}^{\text{lin}}}^2 \leq C M^{-2\al}
	} 
\end{prop}
\begin{proof}
	\eq{
		C \geq \sum_{m=1}^N m^{2\al} |\dotp{f_0}{\psi_m}|^{2}
		\geq \sum_{m>M} m^{2\al} |\dotp{f_0}{\psi_m}|^{2}
		\geq M^{2\al} \sum_{m>M} |\dotp{f_0}{\psi_m}|^{2}
		\geq M^{2\al} \norm{f_0 - f_{0,M}^{\text{lin}}}^2.
	}
\end{proof}

If $\psi_m$ is the discrete Fourier basis defined in~\eqref{eq-dft}, then this discrete Sobolev model~\eqref{eq-discr-sobol} is equivalent to the continuous Sobolev model of Section~\ref{subsec-smooth-class}, up to a discretization error which tends to $0$ as $N$ increase. Choosing $N$ large enough shows that smooth signals and image are thus efficiently denoised by a simple linear projection on the first $M$ element of the Fourier basis. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Non-linear Denoising using Thresholding}
\label{sec-nl-denoising-thresh}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hard Thresholding}
\label{subsec-denoise-hard}

We consider an orthogonal basis $\{\psi_{m}\}_m$ of $\CC^N$, for instance a discrete wavelet basis.
%
The noisy coefficients satisfy
\eql{\label{eq-noisy-transformed}
	\dotp{f}{\psi_m} = \dotp{f_0}{\psi_m} + \dotp{w}{\psi_m}.
}
Since a Gaussian white noise is invariant under an orthogonal transformation, $\dotp{w}{\psi_m}$ is also a Gaussian white noise of variance $\si^2$. If the basis $\{\psi_m\}_m$ is efficient to represent $f_0$, then most of the coefficients $ \dotp{f_0}{\psi_m}$ are close to zero, and one observes a large set of small noisy coefficients, as shown on Figure \ref{fig-wavthresh}. This idea of using thresholding estimator for denoising was first systematically explored by Donoho and Jonhstone \cite{donoho-shrinkage}.

\myfigure{
\tabquatre{
\image{denoising}{.24}{wavthresh-noisy}&
\image{denoising}{.24}{wavthresh-coefs-noisy}&
\image{denoising}{.24}{wavthresh-coef-denoised}&
\image{denoising}{.24}{wavthresh-denoised}\\
$f$ & $\dotp{f}{\psi_{j,n}^\om}$ & $S_T(\dotp{f}{\psi_{j,n}^\om})$ & $\tilde f$
}
}{%
	Denoising using thresholding of wavelet coefficients. %	
}{fig-wavthresh}

A thresholding estimator removes these small amplitude coefficients using a non-linear hard thresholding
\eq{
		\tilde f = \sum_{ |\dotp{f}{\psi_m}|>T } \dotp{f}{\psi_m} \psi_m
		 = \sum_{ m } S_T(\dotp{f}{\psi_m}) \psi_m.
}
where $S_T$ is defined in \eqref{eq-hard-thresh}. This corresponds to the computation of the best $M$-term approximation $\tilde f = f_M$ of the noisy function $f$. Figure \ref{fig-wavthresh} shows that if $T$ is well chose, this non-linear estimator is able to remove most of the noise while maintaining sharp features, which was not the case with linear filtering estimatiors.

% Code \ref{denoising-wavthresh} shows how to implement this thresholding for a 1-D or 2-D wavelet transform.

% \matlab{matlab/denoising-wavthresh.m
% }{Denoising by hard thresholding. Input: noisy image \matvar{f}, coarse scale \matvar{j0}, threshold \matvar{T}, output: denoised image \matvar{f1}.  }{denoising-wavthresh}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Soft Thresholding}

We recall that the hard thresholding operator is defined as
\eql{\label{eq-hard-thresh-denoise}
	S_T(x) = S_T^0(x) = \choice{ x \qifq |x|>T, \\ 0 \qifq |x|\leq T. }
}
This thresholding performs a binary decision that might introduces artifacts. A less aggressive nonlinearity is the soft thresholding
\eql{\label{eq-soft-thresh-denoise}
	 S_T^1( x ) = \max( 1-T/|x|, 0 ) x.
}
Figure \ref{fig-thresholding-hard-vs-soft} shows the 1-D curves of these 1-D non-linear mapping.

\myfigure{
\image{denoising}{.4}{thresholding-hard-vs-soft}
}{%
	Hard and soft thresholding functions. %	
}{fig-thresholding-hard-vs-soft}

For $q=0$ and $q=1$, these thresholding defines two different estimators
\eql{\label{eq-hard-soft-thresh}
	\tilde f^q = \sum_m S_T^q( \dotp{f}{\psi_m} ) \psi_m
}

\myfigure{
\image{denoising}{.6}{wavthresh-2d-hard-vs-soft}
}{%
	Curves of SNR with respect to $T/\si$ for hard and soft thresholding.%	
}{fig-wavthresh-2d-hard-vs-soft}


%%
\paragraph{Coarse scale management.}

The soft thresholded $S_T^1$ introduces a bias since it diminishes the value of large coefficients. 
For wavelet transforms, it tends to introduces unwanted low-frequencies artifacts by modifying coarse scale coefficients. If the coarse scale is $2^{j_0}$, one thus prefers not to threshold the coarse approximation coefficients and use, for instance in 1-D,
\eq{
	\tilde f^1 = \sum_{0 \leq n < 2^{-j_0}} \dotp{f}{\phi_{j_0,n}} \phi_{j_0,n}
	+ \sum_{j=j_0}^0 \sum_{0 \leq n < 2^{-j}} S_T^1(\dotp{f}{\psi_{j_0,n}}) \psi_{j_0,n}.
}
% Code \ref{denoising-softthresh} implements this soft thresholding with coarse scale management.

% \matlab{matlab/denoising-softthresh.m
% }{Denoising by soft thresholding. Input: noisy image \matvar{f}, coarse scale \matvar{j0}, threshold \matvar{T}, output: denoised image \matvar{f1}.  }{denoising-softthresh}


%%
\paragraph{Empirical choice of the threshold.}

Figure \ref{fig-wavthresh-2d-hard-vs-soft} shows the evolution of the SNR with respect to the threshold $T$ for these two estimators, for a natural image $f_0$. For the hard thresholding, the best result is obtained around $T \approx 3\si$, while for the soft thresholding, the optimal choice is around $T\approx 3\si/2$. These results also shows that numerically, for thresholding in orthogonal bases, soft thresholding is slightly superior than hard thresholding on natural signals and images. 

\myfigure{
\tabdeux{
\image{denoising}{.3}{wavthresh-2d-hard}&
\image{denoising}{.3}{wavthresh-2d-soft}\\
20.9dB & 21.8dB
}
}{%
	Comparison of hard (left) and soft (right) thresholding.%	
}{fig-wavthresh-2d}

Although these are experimental conclusions, these results are robust across various natural signals and images, and should be considered as good default parameters. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Minimax Optimality of Thresholding}
\label{subsec-minimax-denoise}

%%
\paragraph{Sparse coefficients estimation.}

To analyze the performance of the estimator, and gives an estimate for the value of $T$, we first assumes that the coefficients
\eq{
	a_{0,m} = \dotp{f_0}{\psi_m} \in \RR^N
}
are sparse, meaning that most of the $a_{0,m}$ are zero, so that its $\lzero$ norm
\eq{
	\norm{a_0}_0 = \#\enscond{m}{a_{0,m} \neq 0}
}
is small. 
As shown in \eqref{eq-noisy-transformed}, noisy coefficients 
\eq{
	\dotp{f}{\psi_m} = a_m = a_{0,m} + z_m
}
are perturbed with an additive Gaussian white noise of variance $\si^2$. Figure \ref{fig-sparse-signal} shows an example of such a noisy sparse signal. 

\myfigure{
\image{denoising}{.45}{sparse-signal}
\image{denoising}{.45}{sparse-signal-noisy}
}{%
	Left: sparse signal $a$, right: noisy signal.%	
}{fig-sparse-signal}


%\myfigure{
%\image{denoising}{.32}{sparse-signal-curve}
%}{
% 	Curve of SNR of thresholding with respect to $T$.%
%}{fig-sparse-signal-curve}

%%
\paragraph{Universal threshold value.}

If 
\eq{
	\umin{ m : a_{0,m} \neq 0 } |a_{0,m}|
}
is large enough, then $\norm{f_0-\tilde f}=\norm{a_0-S_T(a)}$ is minimum for 
\eq{
	T \approx \tau_N = \umax{0 \leq m< N} |z_m|.
}
$\tau_N$ is a random variable that depends on $N$. One can show that its mean is $\si \sqrt{2 \log(N)}$, and that as $N$ increases, its variance tends to zero and $\tau_N$ is highly concentrated close to its mean. Figure \ref{fig-max-gaussian-mean} shows that this is indeed the case numerically.

\myfigure{
\image{denoising}{.6}{max-gaussian-mean}
\image{denoising}{.63}{max-gaussian-std}
}{%
	Empirical estimation of the mean of $Z_n$ (top) and standard deviation of $Z_n$ (bottom)%	
}{fig-max-gaussian-mean}


%%
\paragraph{Asymptotic optimality.}

Donoho and Jonhstone \cite{donoho-shrinkage} have shown that the universal threshold $T = \si \sqrt{2 \log(N)}$ is a good theoretical choice for the denoising of signals that are well approximated non-linearly in $\{\psi_m\}_m$.  The obtain denoising error decay rate with $\si$ can also be shown to be in some sense optimal. 


\begin{thm}\label{thm-nonlinear-denoising}
	We assume that $f_0 \in \RR^N$ has a non-linear approximation error decay that satisfies 
	\eq{
		\foralls M, \quad 
		\norm{f_0 - f_{0,M}^{\text{nlin}} }^2 \leq C M^{-2\be}
		\qwhereq
		f_{0,M}^{\text{nlin}} \eqdef \sum_{r=1}^M \dotp{f_0}{\psi_{m_r}} \psi_{m_r}
	} 
	for some constant $C$, where here $( \dotp{f_0}{\psi_{m_r}} )_r$ are the coefficient sorted by decaying magnitude. 
	%
	Then the non-linear denoising error using~\eqref{eq-ideal-linear-denoise} satisfies
	\eq{
		\EE(\norm{f_0 - \tilde f^q}^2) \leq C' \ln(N) \si^{2-\frac{1}{\be+1/2}}, 
	}
	for some constant $C'$, when choosing $T=\sqrt{2\ln(N)}$, where $\tilde f^q$ is defined in~\eqref{eq-hard-soft-thresh} for $q \in \{0,1\}$.
\end{thm}

This universal threshold choice $T=\sqrt{2\ln(N)}$ is however very conservative since it is guaranteed to remove almost all the noise. In practice, as shown in Figure \ref{fig-wavthresh-2d}, better results are obtained on natural signals and images by using $T \approx 3\si$ and $T \approx 3\si/2$ for hard and soft thresholdings.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Translation Invariant Thresholding Estimators}

%%
\paragraph{Translation invariance.}

Let $f \mapsto \tilde f = \Dd(f)$ by a denoising method, and $f_\tau(x) = f(x-\tau)$ be a translated signal or image for $\tau \in \RR^d$, ($d=1$ or $d=2$).
The denoising is said to be translation invariant at precision $\De$ if
\eq{ 
	\foralls \tau \in \De, \quad \Dd(f) = \Dd(f_\tau)_{-\tau}
}
where $\De$ is a lattice of $\RR^d$. The denser $\De$ is, the more translation invariant the method is.
This corresponds to the fact that $\Dd$ computes with the translation operator.
\begin{center}
	\image{denoising}{.3}{cycle-spin-principle}
\end{center}
Imposing translation invariance for a fine enough set $\De$ is a natural constraint, since intuitively the denoising results should not depend on the location of features in the signal or image. Otherwise, some locations might be favored by the denoising process, which might result in visually unpleasant denoising artifacts.

For denoising by thresholding 
\eq{
	\Dd(f) = \sum_m S_T( \dotp{f}{\psi_m} ) \psi_m.
}
then translation invariance is equivalent to asking that the basis $\{\psi_m\}_m$ is translation invariant at precision $\De$, 
\eq{
	\foralls m, \: \foralls \tau \in \De, \: \exists m, \: \exists \la \in \CC,  \quad \quad (\psi_{m'})_\tau = \la \psi_m
}
where $|\la|=1$.

The Fourier basis is fully translation invariant for $\De=\RR^d$ over $[0,1]^d$ with periodic boundary conditions and the discrete Fourier basis is translation invariant for all interger translations $\De=\{0,\ldots,N_0-1\}^d$ where $N=N_0$ is the number of points in 1-D, and $N=N_0 \times N_0$ is the number of pixels in 2-D.

Unfortunately, an orthogonal wavelet basis 
\eq{
	\{\psi_m = \psi_{j,n} \}_{j,n}
}
is not translation invariant both in the continuous setting or in the discrete setting. For instance, in 1-D, 
\eq{
	(\psi_{j',n'})_{\tau} \notin \{ \psi_{j,n} \} \qforq \tau = 2^{j}/2.
}

%%
\paragraph{Cycle spinning.}

A simple way to turn a denoiser $\De$ into a translation invariant denoiser is to average the result of translated images
\eql{\label{eq-cycle-spinning}
	\Dd_{\text{inv}}(f) = \frac{1}{|\De|} \sum_{\tau \in \De} \Dd( f_{\tau} )_{-\tau}.
}
One easily check that 
\eq{ 
	\foralls \tau \in \De, \quad \Dd_{\text{inv}}(f) = \Dd_{\text{inv}}(f_\tau)_{-\tau}
}
To obtain a translation invariance up to the pixel precision for a data of $N$ samples, one should use a set of $|\De|=N$ translation vectors. To obtain a pixel precision invariance for wavelets, this will result in $O(N^2)$ operations.

Figure \ref{fig-wavthresh-2d-soft-ti} shows the result of applying cycle spinning to an orthogonal hard thresholding denoising using wavelets, where we have used the following translation of the continuous wavelet basis
$\De = \{ 0, 1/N, 2/N, 3/N \}^2$, which corresponds to discrete translation by $\{ 0, 1, 2, 3 \}^2$  on the discretized image.
The complexity of the denoising scheme is thus $16$ wavelet transforms. The translation invariance brings a very large SNR improvement, and significantly reduces the oscillating artifacts of orthogonal thresholding. This is because this artifacts pop-out at random locations when $\tau$ changes, so that the averaging process reduces significantly these artifacts.

Figure \ref{fig-wavthresh-ti} shows that translation invariant hard thresholding does a slightly better job than translation invariant soft thresholding. The situation is thus reversed with respect to thresholding in an orthogonal wavelet basis. % Code \ref{denoising-cyclespin} implement this cycle spinning for a 2-D wavelet thresholding.

\myfigure{
\tabdeux{
\image{denoising}{.3}{wavthresh-2d-soft}&
\image{denoising}{.3}{wavthresh-2d-hard-ti}\\
21.8dB & 23.4dB
}
}{%
	Comparison of wavelet orthogonal soft thresholding (left) and translation invariant wavelet hard thresholding (right). %	
}{fig-wavthresh-2d-soft-ti}


\myfigure{
\image{denoising}{.5}{wavthresh-ti-curves}
}{%
	Curve of SNR with respect to $T/\si$ for translation invariant thresholding. %	
}{fig-wavthresh-ti}


% \matlab{matlab/denoising-cyclespin.m
% }{Denoising by cycle spinning. Input: noisy image \matvar{f}, coarse scale \matvar{j0}, threshold \matvar{T}, number of spins in each direction \matvar{s}, output: denoised image \matvar{f1}.  }{denoising-cyclespin}

%%
\paragraph{Translation invariant wavelet frame.}

An equivalent way to define a translation invariant denoiser is to replace the orthogonal basis $\Bb = \{\psi_m\}$ by a redundant family of translated vectors
\eql{\label{eq-ti-wavframe}
	\Bb_{\text{inv}} = \{ (\psi_m)_\tau \}_{m, \tau \in \De}.
}
One should be careful about the fact that $\Bb_{\text{inv}}$ is not any more an orthogonal basis, but it still enjoy a conservation of energy formula
\eq{
	\norm{f}^2 = \frac{1}{|\De|} \sum_{m, \tau \in \De} |\dotp{f}{(\psi_m)_\tau}|^2
	\qandq
	f = \frac{1}{|\De|} \sum_{m, \tau \in \De} \dotp{f}{(\psi_m)_\tau} (\psi_m)_\tau.
}
This kind of redundant family are called tight frames. 

One can then define a translation invariant thresholding denoising
\eql{\label{eq-ti-thresh}
	\Dd_{\text{inv}}(f) = \frac{1}{|\De|} \sum_{m, \tau \in \De} S_T(\dotp{f}{(\psi_m)_\tau}) (\psi_m)_\tau.
}
This denoising is the same as the cycle spinning denoising defined in \eqref{eq-cycle-spinning}.

The frame $\Bb_{\text{inv}}$ might contain up to $|\De| |\Bb|$ basis element. 
For a discrete basis of signal with $N$ samples, and a translation lattice of $|\De|=N$ vectors, it corresponds to up to $N^2$ elements in $\Bb_{\text{inv}}$. Hopefully, for a hierarchical basis such as a discrete orthogonal wavelet basis, one might have 
\eq{
	(\psi_m)_\tau = (\psi_{m'})_{\tau'}
	\qforq
	m \neq m'
	\qandq
	\tau\neq \tau',
}
so that the number of elements in $\Bb_{\text{inv}}$ might be much smaller than $N^2$. 
For instance, for an orthogonal wavelet basis, one has
\eq{
	(\psi_{j,n})_{k 2^j} = \psi_{j,n+k},
}
so that the number of basis elements is $|\Bb_{\text{inv}}| = N \log_2(N)$ for a 2-D basis, and $3 N \log_2(N)$ for a 2-D basis. The fast translation invariant wavelet transform, also called ``a trou'' wavelet transform, computes all the inner products $\dotp{f}{(\psi_m)_\tau}$ in $O(N \log_2(N))$ operations. Implementing formula \eqref{eq-ti-thresh} is thus much faster than applying the cycle spinning \eqref{eq-cycle-spinning} equivalent formulation.

Translation invariant wavelet coefficients are usually grouped by scales in $\log_2(N)$ (for $d=1$) or by scales and orientations $3 \log_2(N)$ (for $d=2$) sets of coefficients. For instance, for a 2-D translation invariant transform, one consider
\eq{
	\foralls n \in \{0, \ldots, 2^j N_0-1 \}^2, \; 
	\foralls k \in \{0, \ldots, 2^{-j} \}^2, \quad
	d_j^\om[2^{-j} n+k] = \dotp{f}{ (\psi_{j,n})_{k 2^j} }
}
where $\om \in \{V,H,D\}$ is the orientation. Each set $d_j^\om$ has $N$ coefficients and is a band-pass filtered version of the original image $f$, as shown on Figure \ref{fig-tiwav}.


\myfigure{
\tabquatre{
\image{denoising}{.23}{tiwav-2d-image}&
\image{denoising}{.23}{tiwav-2d-j2-h}&
\image{denoising}{.23}{tiwav-2d-j2-v}&
\image{denoising}{.23}{tiwav-2d-j2-d}\\
$f$ & $j=-8, \om=H$ & $j=-8, \om=V$ & $j=-8, \om=D$ \\
&
\image{denoising}{.23}{tiwav-2d-j1-h}&
\image{denoising}{.23}{tiwav-2d-j1-v}&
\image{denoising}{.23}{tiwav-2d-j1-d}\\
& $j=-7, \om=H$ & $j=-7, \om=V$ & $j=-7, \om=D$ 
}
}{%
	Translation invariant wavelet coefficients. %	
}{fig-tiwav}

Figure \ref{fig-tiwav-coefs-thresh} shows how these set of coefficients are hard thresholded by the translation invariant estimator. 

\myfigure{
\image{denoising}{.3}{tiwav-coefs}
\image{denoising}{.3}{tiwav-coefs-thresh}
}{%
	Left: translation invariant wavelet coefficients, for $j=-8, \om=H$, right: tresholded coefficients. %	
}{fig-tiwav-coefs-thresh}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Exotic Thresholdings}

It is possible to devise many thresholding nonlinearities that interpolate between the hard and soft thresholder. We present here two examples, but many more exist in the literature. Depending on the statistical distribution of the wavelet coefficients of the coefficients of $f$ in the basis, these thresholders might produce slightly better results.

%%
\paragraph{Semi-soft thresholding.}

One can define a family of intermediate thresholder that depends on a parameter $\mu>1$
\eq{
	S_T^\th(x) = g_{\frac{1}{1-\th}}(x)
	\qwhereq
	g_\mu(x) = 
	\choice{
		0 \qifq |x|<T\\
		x \qifq |x|>\mu T\\
		\sign(x) \frac{|x|-T}{\mu-1} \text{ otherwise.}
	}
}
One thus recovers the hard thresholding as $S_T^0$ and the soft thresholding as $S_T^1$.
Figure \ref{fig-semisoft} display an example of such a non-linearity.

\myfigure{
\image{denoising}{.45}{semisoft-thresholders}
\image{denoising}{.45}{stein-thresholder}
}{%
	Left: semi-soft thresholder, right: Stein thresholder. %	
}{fig-semisoft}

Figure \ref{fig-semisoft-optimal} shows that a well chosen value of $\mu$ might actually improves over both hard and soft thresholders. The improvement is however hardly noticeable visually.  

\myfigure{
\image{denoising}{.45}{semisoft-optimal-image}
\image{denoising}{.4}{semisoft-optimal-curve}
}{%
	 Left: image of SNR with respect to the parameters $\mu$ and $T/\si$, right: curve of SNR with respect to $\mu$ using the best $T/\si$ for each $\mu$. %	
}{fig-semisoft-optimal}

%%
\paragraph{Stein thresholding.}

The Stein thresholding is defined using a quadratic attenuation of large coefficients
\eq{
 	S_T^{\text{Stein}}(x) = \max\pa{1-\frac{T^2}{|x|^2},0}x.
}
This should be compared with the linear attenuation of the soft thresholding
\eq{
 	S_T^1(x) = \max\pa{1-\frac{T}{|x|},0}x.
}
The advantage of the Stein thresholder with respect to the soft thresholding is that
\eq{
	|S_T^{\text{Stein}}(x)-x| \rightarrow 0
	\quad\text{ whereas }\quad
	|S_T^1(x)-x| \rightarrow T,
}
where $x \rightarrow \pm \infty$. This means that Stein thresholding does not suffer from the bias of soft thresholding.

\myfigure{
\image{denoising}{.45}{stein-optimal-curve}
}{%
	 SNR curves with respect to $T/\si$ for Stein threhsolding. %	
}{fig-stein-optimal-curve}

For translation invariant thresholding, Stein and hard thresholding perform similarly on natural images.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Block Thresholding}

The non-linear thresholding method presented in the previous section are diagonal estimators, since they operate a coefficient-by-coefficient attenuation
\eq{
	\tilde f = \sum_m A_T^q(\dotp{f}{\psi_m}) \dotp{f}{\psi_m} \psi_m
} 
where 
\eq{
	A_T^q(x) = 
	\choice{
		\max(1-x^2/T^2,0) \qforq q=\text{Stein}\\
		\max(1-|x|/T,0) \qforq q=1\text{ (soft)}\\
		1_{|x|>T} \qforq q=0\text{ (hard)}
	}
}
Block thresholding takes advantage of the statistical dependancy of wavelet coefficients, by computing the attenuation factor on block of coefficients. This is especially efficient for natural images, where edges and geometric features create clusters of high magnitude coefficients. Block decisions also help to remove artifacts due to isolated noisy large coefficients in regular areas.

The set of coefficients is divided into disjoint blocks, and for instance for 2-D wavelet coefficients
\eq{
	\{ (j,n,\om) \}_{j,n,\om} = \bigcup_k B_k,
}
where each $B_k$ is a square of $s \times s$ coefficients, where the block size $s$ is a parameter of the method. 
Figure \ref{fig-block-wavcoefs} shows an example of such a block.

The block energy is defined as
\eq{
	B_k = \frac{1}{s^2} \sum_{ m \in B_k } |\dotp{f}{\psi_m}|^2, 
}
and the block thresholding 
\eq{
	\tilde f = \sum_m S_T^{\text{block},q}(\dotp{f}{\psi_m}) \psi_m
}
makes use of the same attenuation for all coefficients within a block
\eq{
	\foralls m \in B_k, \quad S_T^{\text{block},q} (\dotp{f}{\psi_m}) = A_T^q(E_k) \dotp{f}{\psi_m}.
}
for $q \in \{0,1,\text{stein}\}$.
Figure \ref{fig-block-wavcoefs} shows the effect of this block attenuation, and the corresponding denoising result.

\myfigure{
\image{denoising}{.32}{block-wavcoefs}
\image{denoising}{.32}{block-wavcoefs-thresh}
\image{denoising}{.32}{block-denoising}
}{%
	 Left: wavelet coefficients, center: block thresholded coefficients, right: denoised image. %	
}{fig-block-wavcoefs}

Figure \ref{fig-block-optimal-curves}, left, compares the three block thresholding obtained for $q \in \{0,1,\text{stein}\}$. Numerically, on natural images, Stein block thresholding gives the best results.
Figure \ref{fig-block-optimal-curves}, right, compares the block size for the Stein block thresholder. Numerically, for a broad range of images, a value of $s=4$ works well.

\myfigure{
\image{denoising}{.45}{block-optimal-curves}
\image{denoising}{.45}{block-sizeblock}
}{%
	 Curve of SNR with respect to $T/\si$ (left) and comparison of SNR for different block size (right). %	
}{fig-block-optimal-curves}

Figure \ref{fig-comp-thresh} shows a visual comparison of the denoising results. Block stein thresholding of orthogonal wavelet coefficients gives a result nearly as good as a translation invariant wavelet hard thresholding, with a faster algorithm. 
The block thresholding strategy can also be applied to wavelet coefficients in translation invariant tight frame, which produces the best results among all denoisers detailed in this book. 

Code \ref{denoising-block} implement this block thresholding.

\myfigure{
\tabtrois{
\image{denoising}{.32}{wavthresh-2d-hard-ti}&
\image{denoising}{.32}{wavthresh-2d-block}&
\image{denoising}{.32}{wavthresh-2d-block-ti}\\
SNR=23.4dB & 22.8dB & 23.8dB
}
}{%
	 Left: translation invariant wavelet hard thresholding, center: block orthogonal Stein thresholding, 
	 right: block translation invariant Stein thresholding. %	
}{fig-comp-thresh}

One should be aware that more advanced denoisers use complicated statistical models that improves over the methods proposed in this book, see for instance \cite{portilla-denoise}.


% \matlab{matlab/denoising-block.m
% }{Denoising by Stein block thresholding. Input: noisy image \matvar{f}, coarse scale \matvar{j0}, threshold \matvar{T}, size \matvar{s} of the blocks, output: denoised image \matvar{f1}.  }{denoising-block}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data-dependant Noises}
\label{sect-data-dept-noises}

For many imaging devices, the variance of the noise that perturbs $f_{0,n}$ depends on the value of $f_{0,n}$. This is a major departure from the additive noise formation model considered so far. We present here two popular examples of such non-additive models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Poisson Noise}

Many imaging devices sample an image through a photons counting operation. This is for instance the case in digital camera, confocal microscopy, TEP and SPECT tomography.

%%
\paragraph{Poisson model.}

The uncertainty of the measurements for a quantized unknown image $f_{0,n} \in \NN$ is then modeled using a Poisson noise distribution 
\eq{
	f_n \sim \Pp(\la)
	\qwhereq \la=f_{0,n} \in \NN,
}
and where the Poisson distribution $\Pp(\la)$ is defined as
\eq{
	\PP(f_n=k)=\frac{\la^k e^{-\la}}{k!}
}
and thus varies from pixel to pixel. Figure \ref{fig-poisson-distrib} shows examples of Poisson distributions.

\myfigure{
\image{denoising}{.5}{poisson-distrib}
}{%
	 Poisson distributions for various $\la$. %	
}{fig-poisson-distrib}

One has 
\eq{
	\EE(f_n)=\la=f_{0,n}
	\qandq
	\text{Var}(f_n)=\la=f_{0,n}
}
so that the denoising corresponds to estimating the mean of a random vector from a single observation, but the variance now depends on the pixel intensity. This shows that the noise level increase with the intensity of the pixel (more photons are coming to the sensor) but the relative variation $(f_n-f_{0,n})/f_{0,n}$ tends to zero in expectation when $f_{0,n}$ increases.

Figure \ref{fig-poisson-denoising} shows examples of a clean image $f_0$ quantized using different values of $\la_{\max}$ and perturbed with the Poisson noise model.

\myfigure{
\tabquatre{
\image{denoising}{.23}{poisson-denoising-1}&
\image{denoising}{.23}{poisson-denoising-2}&
\image{denoising}{.23}{poisson-denoising-3}&
\image{denoising}{.23}{poisson-denoising-4}\\
$\la_{\max}=5$ & $\la_{\max}=50$ & $\la_{\max}=50$ & $\la_{\max}=100$
}
}{%
	 Noisy image with Poisson noise model, for various $\la_{\max}= \max_n f_{0,n}$. %	
}{fig-poisson-denoising}

%%
\paragraph{Variance stabilization.}

Applying thresholding estimator 
\eq{
	\Dd(f) = \sum_m S_T^q(\dotp{f}{\psi_m}) \psi_m
}
to $f$ might give poor results since the noise level fluctuates from point to point, and thus a single threshold $T$ might not be able to capture these variations. A simple way to improve the thresholding results is to first apply a variance stabilization non-linearity $\phi : \RR \rightarrow \RR$ to the image, so that $\phi(f)$ is as close as possible to an additive Gaussian white noise model
\eql{\label{eq-var-stab}
	\phi(f) \approx \phi(f_0) + w
}
where $w_n \sim \Nn(0,\si)$ is a Gaussian white noise of fixed variance $\si^2$.

Perfect stabilization is impossible, so that \eqref{eq-var-stab} only approximately holds for a limited intensity range of $f_{0,n}$.
Two popular variation stabilization functions for Poisson noise are the Anscombe mapping
\eq{
	\phi(x) = 2\sqrt{x+3/8}
}
and the mapping of Freeman and Tukey 
\eq{
	\phi(x) = \sqrt{x+1}+\sqrt{x}.
}
Figure \ref{fig-variance-stabilization} shows the effect of these variance stabilizations on the variance of $\phi(f)$.

\myfigure{
\image{denoising}{.5}{variance-stabilization}
}{%
	 Comparison of variariance stabilization: display of $\text{Var}(\phi(f_n))$ as a function of $f_{0,n}$. %	
}{fig-variance-stabilization}

A variance stabilized denoiser is defined as
\eq{
	\De^{\text{stab},q}(f) =  \phi^{-1}( \sum_m S_T^q(\dotp{\phi(f)}{\psi_m}) \psi_m )
}
where $\phi^{-1}$ is the inverse mapping of $\phi$.

Figure \ref{fig-poisson-wav} shows that for moderate intensity range, variance stabilization improves over non-stabilized denoising.

\myfigure{
\tabtrois{
\image{denoising}{.32}{poisson-wav-noisy}&
\image{denoising}{.32}{poisson-wav-unstab}&
\image{denoising}{.32}{poisson-wav-stab}
}
}{%
	Left: noisy image, center: denoising without variance stabilization, right: denoising after variance stabilization. %	
}{fig-poisson-wav}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multiplicative Noise}

%%
\paragraph{Multiplicative image formation.}

A multiplicative noise model assumes that
\eq{
	f_n = f_{0,n} w_n
}
where $w$ is a realization of a random vector with  $\EE(w)=1$.
Once again, the noise level depends on the pixel value
\eq{
	\EE(f_n) = f_{0,n}
	\qandq
	\text{Var}(f_n) = f_{0,n}^2 \si^2 
	\qwhereq 
	\si^2 = \text{Var}(w).	
}
Such a mutiplicative noise is a good model for SAR satellite imaging, where $f$ is obtained by averaging $S$ images
\eq{
	\foralls 0 \leq s < K, \quad f^{(s)}_n = f_{0,n} w^{(s)}_n + r^{(s)}_n
}
where $r^{(s)}$ is a Gaussian white noise, and $w^{(s)}_n$ is distributed according to a one-sided exponential distribution
\eq{
	\Pp(w^{(s)}_n=x) \propto e^{-x} \: \mathbb{I}_{x>0}.
}
For $K$ large enough, averaging the images cancels the additive noise and one obtains
\eq{
	f_n = \frac{1}{K} \sum_{s=1}^K f^{(s)}_n \approx f_{0,n} w_n
}
where $w$ is distributed according to a Gamma distribution 
\eq{
	w \sim \Gamma(\si=K^{-\frac{1}{2}}, \mu=1)
	\qwhereq
	\PP(w=x) \propto x^{K-1} e^{-K x},
}
One should note that increasing the value of $K$ reduces the overall noise level.

\myfigure{
\tabquatre{
\image{denoising}{.23}{multnoise-1}&
\image{denoising}{.23}{multnoise-2}&
\image{denoising}{.23}{multnoise-3}&
\image{denoising}{.23}{multnoise-4}
}
}{%
	 Noisy images with multiplicative noise, with varying $\si$. %	
}{fig-multnoise}

Figure \ref{multnoise} shows an example of such image formation for a varying number $K=1/\si^2$ of averaged images.

A simple variance stabilization transform is
\eq{
	\phi(x)=\log(x) - c
}
where
\eq{
	c = \text{E}(\log(w)) = \psi(K) - \log(K)
	\qwhereq
	\psi(x) = \Ga'(x)/\Ga(x)
}
and where $\Ga$ is the Gamma function that generalizes the factorial function to non-integer.
One thus has
\eq{
	\phi(f)_n = \phi(f_0)_n + z_n,
}
where $z_n=\log(w)-c$ is a zero-mean additive noise. 

\myfigure{
\image{denoising}{.45}{multnoise-hist-unstab}
\image{denoising}{.45}{multnoise-hist-stab}
}{%
	 Histogram of multiplicative noise before (left) and after (right) stabilization. %	
}{fig-multnoise-hist-unstab}

Figure \ref{fig-multnoise-hist-unstab} shows the effect of this variance stabilization on the repartition of $w$ and $z$.

Figure \ref{fig-multnoise-wav} shows that for moderate noise level $\si$, variance stabilization improves over non-stabilized denoising.

\myfigure{
\tabtrois{
\image{denoising}{.32}{multnoise-wav-noisy}&
\image{denoising}{.32}{multnoise-wav-stab}&
\image{denoising}{.32}{multnoise-wav-unstab}
}
}{%
	Left: noisy image, center: denoising without variance stabilization, right: denoising after variance stabilization. %		
}{fig-multnoise-wav}




