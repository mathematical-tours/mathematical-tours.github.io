% !TEX root = ../FundationsDataScience.tex

\chapter{Sparse Regularization}
\label{chap-sparse-regul}

Ref \cite{mallat2008wavelet,starck2015sparse,scherzer2009variational}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sparsity Priors}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ideal sparsity prior.}

As detailed in Chapter \ref{chap-approximation}, it is possible to use an orthogonal basis $\Bb = \{ \psi_m \}_m$ to efficiently approximate an image $f$ in a given class $f \in \Th$ with a few atoms from $\Bb$. 

To measure the complexity of an approximation with $\Bb$, we consider the $\lzero$ prior, which counts the number of non-zero coefficients in $\Bb$
\eq{
	\Jz(f) \eqdef \#\enscond{m}{\dotp{f}{\psi_m} \neq 0}
	\qwhereq
	x_m = \dotp{f}{\psi_m}.
}
One often also denote it as the $\lzero$ ``pseudo-norm''
\eq{
	\normz{x} \eqdef \Jz(f).
}
which we treat here as an ideal sparsity measure for the coefficients $x$ of $f$ in $\Bb$.

Natural images are not exactly composed of a few atoms, but they can be well approximated by a function $f_M$ with a small ideal sparsity $M=\Jz(f)$. In particular, the best $M$-term approximation defined in \eqref{eq-nonlinear-approx} is defined by
\eq{
	f_M = \sum_{ |\dotp{f}{\psi_m}|>T } \dotp{f}{\psi_m} \psi_m
	\qwhereq
	M = \#\enscond{m}{|\dotp{f}{\psi_m}|>T}.
}
As detailed in Section \ref{sec-signal-models}, discontinuous images with bounded variation have a fast decay of the approximation error $\norm{f-f_M}$. Natural images $f$ are well approximated by images with a small value of the ideal sparsity prior $\Jz$.

Figure \ref{fig-sparsity-wav} shows an examples of decomposition of a natural image in a wavelet basis, $\psi_m = \psi_{j,n}^\om$ $m=(j,n,\om)$. This shows that most $\dotp{f}{\psi_m}$ are small, and hence the decomposition is quite sparse.

\myfigure{
\tabdeux{
\image{variational}{.3}{sparsity-image}&
\image{variational}{.3}{sparsity-wav}\\
Image $f$ & Coefficients $\dotp{f}{\psi_m}$
}
}{%
	Wavelet coefficients of natural images are relatively sparse.
}{fig-sparsity-wav}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Convex relaxation}

Unfortunately, the ideal sparsity prior $\Jz$ is difficult to handle numerically because $\Jz(f)$ is not a convex function of $f$. For instance, if $f$ and $g$ have non-intersecting supports of there coefficients in $\Bb$, then $\Jz( (f+g)/2 ) = \Jz(f)+\Jz(g)$, which shows the highly non-convex behavior of $\Jz$.

This ideal sparsity $\Jz$ is thus not amenable to minimization, which is an issue to solve general inverse problems considered in Section \ref{chap-invpbm}. 

We consider a family of $\ell^q$ priors for $q>0$, intended to approximate the ideal prior $\Jz$ 
\eq{
	J_q(f) = \displaystyle \sum_m |\dotp{f}{\psi_m}|^q.
}
As shown in Figure \ref{fig-sparsity-lq}, the unit balls in $\RR^2$ associated to these priors are shrinking toward the axes, which corresponds to the unit ball for the $\lzero$ pseudo norm. In some sense, the $J_q$ priors are becoming closer to $\Jz$ as $q$ tends to zero, and thus $J_q$ favors sparsity for small $q$.


\myfigure{
\image{variational}{1}{sparsity-lq-balls}
}{%
	$\ell^q$ balls $\enscond{x}{J_q(x) \leq 1}$ for varying $q$.
}{fig-sparsity-lq}


The prior $J_q$ is convex if and only if $q \geq 1$. To reach the highest degree of sparsity while using a convex prior, we consider the $\lun$ sparsity prior $\Ju$, which is thus defined as 
\eql{\label{eq-dfn-lun-prior}
	\Ju(f) = \norm{(\dotp{f}{\psi_m})}_1 = \sum_m |\dotp{f}{\psi_m}|.
}
In the following, we consider discrete orthogonal bases $\Bb = \{\psi_m\}_{m=0}^{N-1}$ of $\RR^N$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sparse Regularization and Thresholding}

Given some orthogonal basis $\{\psi_m\}_m$ of $\RR^N$, the denoising by regularization \eqref{eq-regul-denoising} is written using the sparsity $\Jz$ and $\Ju$ as 
\eq{
	f^\star = \uargmin{g \in \RR^N} \frac{1}{2} \norm{f-g}^2 + \la J_q(f)
}
for $q=0$ or $q=1$. It can be re-written in the orthogonal basis as
\eq{
	f^\star = \sum_m x^\star_m \psi_m
}
\eq{
	\qwhereq
	x^{\star} = \uargmin{y \in \RR^N} \sum_m \frac{1}{2} |x_m-y_m|^2 + \la |y_m|^q
}
where $x_m \eqdef \dotp{f}{\psi_m}$, $y_m \eqdef \dotp{g}{\psi_m}$, and where we use the following slight abuse of notation for $q=0$
\eq{
	\foralls u \in \RR, \quad
	|u|^0=
	\choice{
		0 \qifq u=0, \\ 1 \quad \text{otherwise.} 
	}
}
Each coefficients of the denoised image is the solution of a 1-D optimization problem
\eql{\label{eq-var-1D-spars}
	x_m^\star = \uargmin{u \in \RR}  \frac{1}{2} |x_m-u|^2 + \la |u|^q
}
and the following proposition this optimization is solved exactly in closed form using thresholding.

\begin{prop}\label{prop-equiv-sparse-thresh}
One has
\eql{\label{eq-equiv-sparse-thresh}
	x^{\star}_m = S_T^q( x_m )
	\qwhereq 
	\choice{
		T = \sqrt{2 \la} \qforq q=0,\\
		T = \la \qforq q=1,
	}
}
where 
\eql{\label{eq-hard-thresh}
	\foralls u \in \RR, \quad
	S_T^0(u) \eqdef
	\choice{
		0 \qifq |u| < T, \\
		u \quad\text{otherwise}	
	}
} 
is the hard thresholding introduced in~\eqref{eq-hard-thresh-denoise}, and 
\eql{\label{eq-soft-thresh}
	\foralls u \in \RR, \quad
	S_T^1(u) \eqdef  \sign(u) ( |u|-T )_+
} 
is the soft thresholding introduced in~\eqref{eq-soft-thresh-denoise}.
\end{prop}

	
\begin{proof}
	One needs to solve~\eqref{eq-var-1D-spars}. Figure~\ref{fig-varspars}, left shows the function $\norm{x-y}^2+T^2 \norm{x}_0$, and the minimum is clearly at $x=0$ when $T \geq y$, and at $x=y$ otherwise. This is thus a hard thresholding with threshold $T^2=2\la$. 
	%
	Figure~\eqref{fig-varspars}, right, shows the evolution with $\la$ of the function $\frac{1}{2}\norm{x-y}^2+\la |x|$. 
	% 
	For $x>0$, one has $F'(x)=x-y+\la$ wich is $0$ at $x=y-\la$.
	The minimum is at $x=y-\la$ for $\la \leq y$, and stays at $0$ for all $\la>y$. 
\end{proof}

\myfigure{
\image{sparse-reg}{.19}{var-l0}
%
\image{sparse-reg}{.19}{var-l1-1}
\image{sparse-reg}{.19}{var-l1-2}
\image{sparse-reg}{.19}{var-l1-3}
\image{sparse-reg}{.19}{var-l1-4}
}{%	
	Leftmost: function $\norm{\cdot-y}^2+T^2 \norm{\cdot}_0$ .
	%
	Others: evolution with $\la$ of the function $F(x) \eqdef \frac{1}{2}\norm{\cdot-y}^2+\la |\cdot|$. %	
}{fig-varspars}




One thus has
\eq{
	f_{\la,q} = \sum_m S_T^q( \dotp{f}{\psi_m} ) \psi_m.
}
As detailed in Section \ref{sec-nl-denoising-thresh}, these denoising methods has the advantage that the threshold is simple to set for Gaussian white noise $w$ of variance $\si^2$. Theoretical values indicated that $T = \sqrt{2\log(N)} \si$ is asymptotically optimal, see Section \ref{subsec-minimax-denoise}. In practice, one should choose $T \approx 3\si$ for hard thresholding ($\lzero$ regularization), and $T \approx 3\si/2$ for soft thresholding ($\lun$ regularization), see Figure \ref{fig-wavthresh-2d}.



% Bayesian interpretation:  $\dotp{f}{\psi_m}$ i.i.d. $ \sim$ generalized Gaussian


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sparse Regularization of Inverse Problems}
\label{sec-sparse-ip}

Sparse $\lun$ regularization in an orthogonal basis $\{\psi_m\}_m$ of $\RR^N$ makes use of the $\Ju$ prior defined in \eqref{eq-dfn-lun-prior}, so that the inversion is obtained by solving the following convex program
\eql{\label{eq-bpdn}
	f_\la \in \uargmin{f \in \RR^N} \frac{1}{2}\norm{y-\Phi f}^2 + \la \sum_m |\dotp{f}{\psi_m}|.
}
This corresponds to the basis pursuit denoising for sparse approximation introduced by Chen, Donoho and Saunders in \cite{chen-basis-pursuit}. The resolution of \eqref{eq-bpdn} can be perform using an iterative thresholding algorithm as detailed in Section \ref{subsec-proximal-ip}. 

%%%
\paragraph{Analysis vs. synthesis priors.}

When the set of atoms $\Psi = \{\psi_m\}_{m=1}^Q$ is non-orthognal (and might even be redundant in the case $Q>N$), there is two distincts way to generalizes problem~\eqref{eq-bpdn}, which we formulate as in~\eqref{}, by introducing a generic convex prior $J$
\eql{\label{eq-regul-generic}
	f_\la \in \uargmin{f \in \RR^N} \frac{1}{2}\norm{y-\Phi f}^2 + \la J(f).
}
In the following, with a slight abuse of notation, we denote the ``analysis'' and ``synthesis'' operator as
\eq{
	\Psi : x \in \RR^Q \mapsto \Psi x = \sum_m x_m \psi_m
	\qandq
	\Psi^* : f \in \RR^N \mapsto ( \dotp{f}{\psi_m} )_{m=1}^Q \in \RR^Q.
}

The so-called analysis-type prior is simply measuring the sparsity of the correlations with the atoms in the dictionary
\eql{\label{eq-sparsity-analysis}
	J_1^{\text{A}}(f) \eqdef \sum_m |\dotp{f}{\psi_m}| = \norm{\Psi^* f}_1.
}
The so-called synthesis-type priori in contrast measure the sparsity of the sparsest expansion of $f$ in $\Psi$, i.e.
\eql{\label{eq-sparsity-synthesis}
	J_1^{\text{S}}(f) \eqdef \umin{x \in \RR^q, \Psi x = f} \norm{x}_1.  
}
While the analysis regularization~\eqref{eq-sparsity-analysis} seems simpler to handle, it is actually the contrary. Solving~\eqref{eq-regul-generic} with $J=J_1^{\text{A}}$ is in fact quite involved, and necessitate typically primal-dual algorithm as detailed in Chapter~\ref{chap-conv-duality}. Furthermore, the theoretical study of the performance of the resulting regularization method is mostly an open problem. 

We thus now focus on the synthesis regularization problem $J=J_1^{\text{S}}$, and we re-write~\eqref{eq-regul-generic} conveniently as $f_\la = \Psi x_\la$ where $x_\la$ is any solution of the following Basis Pursuit Denoising problem
\eql{\label{eq-lasso-lagr-ip}
	x_\la \in \uargmin{x \in \RR^Q} \frac{1}{2\la} \norm{y-Ax}^2 + \norm{x}_1
} 
where we introduced the following matrix
\eq{
	A \eqdef \Phi \Psi \in \RR^{P \times Q}. 
}
As $\la \rightarrow 0$, we consider the following limit constrained problem
\eql{\label{eq-lasso-constr-ip}
	x^\star = \uargmin{A x = y} \norm{x}_1
} 
and the signal is recovered as $f^\star=\Psi x^\star \in \RR^N$.


% This can be recasted as a convex linear program, which can in turn by solved by various solver such as simplex, interior points, or Douglas-Rachford iterations.

\myfigure{
\tabtrois{
\image{invpbm}{.3}{optim-geom-l1}&
\image{invpbm}{.3}{optim-geom-l2}&
\image{invpbm}{.3}{optim-geom-l1noisy}\\
$\umin{\Phi f = y} \normu{\Psi^* f}$ &
$\umin{\Phi f = y} \norm{f}^2$ &
$\umin{\norm{\Phi f = y} \leq \epsilon} \normu{\Psi^* f}$ 
}
}{%
	Geometry of convex optimizations. %
}{fig-optim-geom}


 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Iterative Soft Thresholding Algorithm}
\label{subsec-proximal-ip}

This section details an iterative algorithm that compute a solution of~\eqref{eq-lasso-lagr-ip}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Noiseless Recovery as a Linear Program}
\label{eq-linearprog-lasso}

Before detailing this methods, which only deal with the case $\la>0$, let us note that in the noiseless settig, $\la=0$ and~\eqref{eq-lasso-constr-ip} is actually equivalent to a linear program. Indeed, decomposing $a=x_+-x_-$ with $(x_+,x_-) \in (\RR_+^Q)^2$, one has
\eql{\label{eq-lin-prog-lasso}
	x^\star = \uargmin{(x_+,x_-) \in (\RR_+^Q)^2} \enscond{ \dotp{x_+}{\ones_Q}+\dotp{x_-}{\ones_Q} }{ y = A(x_+ - x_-) }.
}
which is a linear program. For small to medium scale problem ($Q$ of the order of a few thousands) it can be solved using the simplex algorithm or interior point methods. 
%
For large scale problems such as those encountered in imaging or machine learning, this is not possible, and one has to ressort to simpler first order schemes. A possible option is the Douglas-Rachford splitting scheme, which is detailed in Section~\ref{}.
%
Let us however stress that the constrained problem~\eqref{eq-lasso-constr-ip}, because of its polyhedral (linear) nature, is in fact harder to solve than the penalized problem~\eqref{eq-lasso-lagr-ip} that we now target. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Projected Gradient Descent for $\ell^1$.}

As a first practical example to solve~\eqref{eq-lasso-lagr-ip}, we will show how to use the projected gradient descent method, which is analyzed in detailed in Section~\ref{sec-proj-grad}.
%
Similarly to~\eqref{eq-lin-prog-lasso}, we remap~\eqref{eq-lasso-lagr-ip} as the resolution of a constraint minimization problem of the form~\eqref{eq-constr} where here $\Cc$ is a positivity constraint and
\eq{
	u=(u_+,u_-) \in (\RR^Q)^2, \quad
	\Cc=(\RR_+^Q)^2, 
	\qandq
	\Ee(u) = \frac{1}{2}\norm{ \Phi(u_+-u_-) - y }^2 + \la \dotp{u_+}{\ones_Q} +\la \dotp{u_-}{\ones_Q}.
}
The projection on $\Cc$ is here simple to compute
\eq{
	\Proj_{(\RR_+^Q)^2}(u_+,u_-) = ((u_+)_\oplus,(u_-)_\oplus)
	\qwhereq
	(r)_\oplus \eqdef \max(r,0), 
}
and the gradient reads
\eq{
	\nabla \Ee(u_+,u_-) =  ( \eta + \la \ones_Q , -\eta + \la \ones_Q) 
	\qwhereq
	\eta = \Phi^*( \Phi(u_+-u_-) - y )
}

Denoting $\it{u} = (\it{u_+},\it{u_-})$ and $\it{x} \eqdef \it{u_+}-\it{u_-}$, the iterate of the projected gradient descent algorithm~\eqref{eq-proj-grad-desc} read
\eq{
	\iit{u_+} \eqdef \pa{ \it{u_+} - \tau_\ell ( \it{\eta} + \la  ) }_\oplus
	\qandq
	\iit{u_-} \eqdef \pa{ \it{u_-} - \tau_\ell ( -\it{\eta} + \la  ) }_\oplus
} 
where $\it{\eta} \eqdef \Phi^*( \Phi \it{x} - y )$.

Theorem~\ref{thm-proj-grad} ensures that $\it{u} \rightarrow u^\star$ a solution to~\eqref{eq-constr} if 
\eq{
	\foralls \ell, \quad
	0 < \tau_{\min} <  \tau_\ell < \tau_{\max} < \frac{2}{\norm{\Phi}^2},
}
and thus $\it{x} \rightarrow x^\star = u_+^\star-u_-^\star$ which is thus a solution to~\eqref{eq-lasso-lagr-ip}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Iterative Soft Thresholding and Forward Backward}
\label{sec-ista}

A drawback of this projected gradient descent scheme is that it necessitate to store $2Q$ coefficients. A closely related method, which comes with exactly the same convergence guarantees and rate, is the so called ``iterative soft thresholding algorithm'' (ISTA). 
%
This algorithm was derived by several authors, among which \cite{figueiredo-nowak-em,daubechies-iterated}, and belongs to the general family of forward-backward splitting in proximal iterations \cite{combettes-proximal}, which we detail in Section~\ref{sec-fb}.

For the sake of simplicity, let us derive this algorithm in the special case of $\ell^1$ by surrogate function minimization. 
%
We aim at minimizing~\eqref{eq-bpdn} 
\eq{
	\Ee(x) \eqdef \frac{1}{2} \norm{y-Ax}^2 + \la \norm{x}_1
}
and we introduce for any fixed $x'$ the function 
\eq{
	\Ee_\tau(x,x') \eqdef \Ee(x) - \frac{1}{2}\norm{Ax-Ax'}^2 + \frac{1}{2\tau}\norm{x-x'}^2.
}
We notice that $\Ee(x,x)=0$ and one has 
\eq{
	K(x,x') \eqdef - \frac{1}{2}\norm{Ax-Ax'}^2 + \frac{1}{2\tau}\norm{x-x'} = 
	\frac{1}{2}\dotp{ \pa{\frac{1}{\tau}\Id_N-A^*A} (x-x') }{x-x'}.
}
This quantity $K(x,x')$ is positive if $\la_{\max}(A^*A) \leq 1/\tau$ (maximum eigenvalue), i.e. $\tau \leq 1/\norm{A}_{\text{op}}^2$, where we recall that $\norm{A}_{\text{op}} = \si_{\max}(A)$ is the operator (algebra) norm.
%
This shows that $\Ee_\tau(x,x')$ is a valid surrogate functional, in the sense that
\eq{
	\Ee(x) \leq \Ee_\tau(x,x'), \quad
	\Ee_\tau(x,x')=0, \qandq
	\Ee(\cdot)-\Ee_\tau(\cdot,x') \text{ is smooth.} 
}
This leads to define
\eql{\label{eq-ista-surrog}
	\iit{x} \eqdef \uargmin{x} \Ee_{\tau_\ell}(x,\it{x})
}
which by construction satisfies 
\eq{
	\Ee(\iit{x}) \leq \Ee(\it{x}).
}

\begin{prop}
	The iterates $\it{x}$ defined by~\eqref{eq-ista-surrog} satisfy
	\eql{\label{eq-ista}
		\iit{x} = \Ss^1_{\la\tau_\ell}\pa{
			\it{x} - \tau_\ell A^*( A \it{x} - y ) 
		}
	}
	where $\Ss^1_\la(x) = (  S^1_\la(x_m) )_m$ where $S^1_\la(r) = \sign(r) (|r|-\la)_\oplus$ is the soft thresholding operator defined in~\eqref{eq-soft-thresh}. 
\end{prop}
\begin{proof}
	One has 
	\begin{align*}
		\Ee_\tau(x,x') &= \frac{1}{2}\norm{Ax-y}^2 - \frac{1}{2}\norm{Ax-Ax'}^2 + \frac{1}{2\tau}\norm{x-x'}^2 + \la \norm{x}_1 \\
			& = C + \frac{1}{2}\norm{Ax}^2-\frac{1}{2}\norm{Ax}^2+\frac{1}{2\tau}\norm{x}^2
			- \dotp{Ax}{y} + \dotp{Ax}{Ax'} - \frac{1}{\tau}\dotp{x}{x'}
			+ \la \norm{x}_1 \\
			& = C + \frac{1}{2\tau}\norm{x}^2 + \dotp{x}{ -A^*y+AA^*x'-\frac{1}{\tau}x' } + \la \norm{x}_1 \\
			&= C' + \frac{1}{\tau} \pa{
				\norm{ x - \pa{ x'-\tau A^*(Ax'-y) } }^2 + \tau \la \norm{x}_1.
			}
	\end{align*}
	Proposition~\eqref{prop-equiv-sparse-thresh} shows that the minimizer of $\Ee_\tau(x,x')$ is thus 
	indeed $\Ss^1_{\la\tau}( x' - \tau_\ell A^*( A x' - y ) )$.
\end{proof}

Of course, these iterations~\eqref{eq-ista} are the same as the FB iterates~\eqref{eq-fb-split}, when, for the special case~\eqref{eq-bpdn}, one can consider a splitting of the form~\eqref{eq-fb-split} defining
\eql{\label{eq-bpdn-func}
	\Ff=\frac{1}{2} \norm{A \cdot - y}^2
	\qandq 
	\Gg=\la \norm{\cdot}_1. 
}
%
In the case~\eqref{eq-bpdn-func}, Proposition~\eqref{prop-equiv-sparse-thresh} shows that $\Prox_{\rho J}$ is the soft thresholding.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Example: Sparse Deconvolution}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sparse Spikes Deconvolution}

Sparse spikes deconvolution makes use of sparsity in the spacial domain, which corresponds to the orthogonal basis of Diracs $\psi_m[n] = \de[n-m]$. This sparsity was first introduced in the seismic imaging community \cite{}, where the signal $f_0$ represent the change of density in the underground and is assumed to be composed of a few Diracs impulse.

In a simplified linearized 1D set-up, ignoring multiple reflexions, the acquisition of underground data $f_0$ is modeled as a convolution $y=h \star f_0 + w$, where $h$ is a so-called ``wavelet'' signal sent in the ground. This should not be confounded with the construction of orthogonal wavelet bases detailed in Chapter \ref{chap-wavelets}, although the term ``wavelet'' originally comes from seismic imaging.

The wavelet filter $h$ is typically a band pass signal that perform a tradeoff between space and frequency concentration especially tailored for seismic exploration. Figure \eqref{fig-spikes-lun} shows a typical wavelet that is a second derivative of a Gaussian, together with its Fourier transform. This shows the large amount of information removed from $f$ during the imaging process.

% \myfigure{
% \image{invpbm}{.6}{seismic-data}
% }{%%
%	2D+t seismic data. %	
% }{fig-seismic-data}

The sparse $\lun$ regularization in the Dirac basis reads 
\eq{
	 f^\star = \uargmin{f \in \RR^N} \frac{1}{2} \norm{f \star h - y}^2 + \la \sum_m |f_m|.
}
Figure \ref{fig-spikes-lun} shows the result of $\lun$ minimization for a well chosen $\la$ parameter, that was optimized in an oracle manner to minimize the error $\norm{f^\star-f_0}$.


\myfigure{
\tabdeux{
\image{invpbm}{.4}{spikes-filter}&
\image{invpbm}{.4}{spikes-filter-fourier}\\
$h$ & $\hat h$ \\
% \image{invpbm}{.3}{spikes-filter-fourier-inv}&
\image{invpbm}{.4}{spikes-signal}&
\image{invpbm}{.4}{spikes-observations}\\
$f_0$ & $y=h \star f + w$ \\ 
\image{invpbm}{.4}{spikes-pinv}&
\image{invpbm}{.4}{spikes-l1}\\
$f^+$ & $f^\star$ 
}
}{%
	Pseudo-inverse and $\lun$ sparse spikes deconvolution. %	
}{fig-spikes-lun}

The iterative soft thresholding for sparse spikes inversion iterates
\eq{
	\tilde f^{(k)} = f^{(k)} - \tau h \star ( h \star f^{(k)}-y )
}
and
\eq{
	f^{(k+1)}_m = S_{\la \tau}^{1}(\tilde f^{(k)}_m )
}
where the step size should obeys 
\eq{
	\tau < 2/\norm{\Phi^* \Phi} = 2 / \umax{\om} |\hat h(\om)|^2
}
to guarantee convergence. Figure \ref{fig-spikes-ista-decay} shows the progressive convergence of the algorithm, both in term of energy minimization and iterates. Since the energy is not strictly convex, we note that convergence in energy is not enough to guarantee convergence of the algorithm.


\myfigure{
\tabdeux{
\image{invpbm}{.4}{spikes-ista-decay-conv}&
\image{invpbm}{.4}{spikes-ista-decay-energy} \\
%\image{invpbm}{.3}{spikes-snr-lambda}\\
$\log_{10}( E(f^{(k)})/E(f^\star)-1 )$ &
$\log_{10}( \norm{f^{(k)}-f^\star}/\norm{f_0} )$
}
}{%
	Decay of the energy and convergence through the iterative thresholding iterations. %	
}{fig-spikes-ista-decay}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sparse Wavelets Deconvolution}


Signal and image acquired by camera always contain some amount of blur because of objects being out of focus, movements in the scene during exposure, and diffraction. A simplifying assumption assumes a spatially invariant blur, so that $\Phi$ is a convolution
\eq{
	y = f_0 \star h + w.
}
In the following, we consider $h$ to be a Gaussian filter of width $\mu > 0$. The number of effective measurements can thus be considered to be $P \sim 1/\mu$, since $\Phi$ nearly set to 0 large enough Fourier frequencies. Table \ref{inverse-deconv} details the implementation of the sparse deconvolution algorithm.

Figures \ref{fig-deconvolution-1d} and \ref{fig-deconvolution-2d} shows examples of signal and image acquisition with Gaussian blur. 

Sobolev regularization \eqref{eq-sobol-reg-fourier} improves over $\ldeux$ regularization \eqref{eq-l2-reg-fourier} because it introduces an uniform smoothing that reduces noise artifact. It however fail to recover sharp edge and thus does a poor job in inverting the operator. To recover sharper transition and edges, one can use either a TV regularization or a sparsity in an orthogonal wavelet basis. 

Figure \ref{fig-deconvolution-1d} shows the improvement obtained in 1D with wavelets with respect to Sobolev. Figure \ref{fig-deconvolution-2d} shows that this improvement is also visible for image deblurring. To obtain a better result with fewer artifact, one can replace the soft thresholding in orthogonal wavelets in during the iteration \eqref{eq-ista-2} by a thresholding in a translation invariant tight frame as defined in \eqref{eq-ti-wavframe}.

\myfigure{
\tabdeux{
%\image{invpbm}{.3}{deconv1d-filter-fourier}&
\image{invpbm}{.4}{deconv1d-signal}&
\image{invpbm}{.4}{deconv1d-filter} \\
Signal $f_0$ & Filter $h$ \\
\image{invpbm}{.4}{deconv1d-observations}&
\image{invpbm}{.4}{deconv1d-l1}\\
Observation $y=h\star f_0+w$ & $\lun$ recovery $f^\star$
}
}{%
	Sparse 1D deconvolution using orthogonal wavelets. %	
}{fig-deconvolution-1d}

% \image{invpbm}{.3}{deconv1d-snr}

%%


\myfigure{
\tabtrois{
\image{invpbm}{.3}{deconv2d-image}&
\image{invpbm}{.3}{deconv2d-observations}&
\image{invpbm}{.3}{deconv2d-l2}\\
Image $f_0$ & Observations $y=h\star f_0+w$ & $\ldeux$ regularization \\
 & SNR=?dB & SNR=?dB  \\
\image{invpbm}{.3}{deconv2d-sob}&
\image{invpbm}{.3}{deconv2d-l1}&
\image{invpbm}{.3}{deconv2d-l1inv}\\
Sobolev regularization & $\lun$ regularization & $\lun$ invariant \\
SNR=?dB & SNR=?dB & SNR=?dB 
}
}{%
	Image deconvolution. %	
}{fig-deconvolution-2d}

Figure \ref{fig-deconvolution-2d-snr} shows the decay of the SNR as a function of the regularization parameter $\la$. This SNR is computed in an oracle manner since it requires the knowledge of $f_0$. The optimal value of $\la$ was used in the reported experiments. 

\myfigure{
\tabtrois{
\image{invpbm}{.3}{deconv2d-l2-snr}&
\image{invpbm}{.3}{deconv2d-sob-snr}&
\image{invpbm}{.3}{deconv2d-l1-snr}\\
$\ldeux$ regularization  &
Sobolev regularization  &
$\lun$ regularization  
}
}{%
	SNR as a function of $\la$. %	
}{fig-deconvolution-2d-snr}


%\matlab{matlab/inverse-deconv.m
%}{
%Deconvolution with $\lun$ regularization in a wavelet basis. The filter $\phi$ is given in \matvar{phi}, the observations in \matvar{y}, the regularization parameter $\la$ is \matvar{lambda}. The solution is given in \matvar{fspars}.
%}{inverse-deconv}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sparse Inpainting}

This section is a follow-up of Section~\ref{sec-inpainting-variational}.

To inpaint using a sparsity prior without noise, we use a small value for $\la$. 
The iterative thresholding algorithm \eqref{eq-ista-2} is written as follow for $\tau=1$,
\eq{
	f^{(k+1)} = \sum_m S_{\la}^{1}( \dotp{ P_y(f^{(k)}) }{\psi_m} )  \psi_m
}
Figure \ref{fig-inpainting-lena} shows the improvevement obtained by the sparse prior over the Sobolev prior if one uses soft thresholding in a translation invariant wavelet frame. 

\myfigure{
\tabtrois{
\image{invpbm}{.3}{inpainting-lena-image}&
\image{invpbm}{.3}{inpainting-lena-observations}& \\
Image $f_0$ & Observation $y=\Phi f_0$ & \\
\image{invpbm}{.3}{inpainting-lena-sob}&
\image{invpbm}{.3}{inpainting-lena-wavortho}&
\image{invpbm}{.3}{inpainting-lena-wavti}\\
Sobolev $f^\star$ & Ortho. wav $f^\star$ & TI. wav $f^\star$ \\
SNR=?dB & SNR=?dB & SNR=?dB 
}
}{%
	Inpainting with Sobolev and sparsity. %
}{fig-inpainting-lena}

