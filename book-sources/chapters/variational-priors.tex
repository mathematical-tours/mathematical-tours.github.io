% !TEX root = ../FundationsDataScience.tex

\chapter{Variational Priors and Regularization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sobolev and Total Variation Priors}

The simplest prior are obtained by integrating local differential quantity over the image. They corresponds to norms in functional spaces that imposes some smoothness on the signal of the image. We detail here the Sobolev and the total variation priors, that are the most popular in image processing.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Continuous Priors}

In the following, we consider either continuous functions $f \in \Ldeux([0,1]^2)$ or discrete vectors $f \in \RR^N$, and consider continuous priors and there discrete counterparts in Section \ref{subsec-disc-priors}. 

%%
\paragraph{Sobolev prior.}

The prior energy $J(f) \in \RR$ is intended to be low for images in a class $f \in \Theta$. The class of uniformly smooth functions detailed in Section \ref{subsec-smooth-class} corresponds to functions in Sobolev spaces. A simple prior derived from this Sobolev class is thus
\eql{\label{eq-dfn-sob-energy}
	\Jsob(f)  = \frac{1}{2}\norm{f}_{\text{Sob}}^2 = \frac{1}{2} \int \norm{\nabla f(x)}^2 \d x,
}
where $\nabla f$ is the gradient in the sense of distributions. 

%%
\paragraph{Total variation prior.}

To take into account discontinuities in images, one considers a total variation energy, introduced in Section \ref{subsec-bv-model}. It  was introduced for image denoising by Rudin, Osher and Fatemi \cite{rudin-tv}

The total variation of a smooth image $f$ is defined as 
\eql{\label{eq-dfn-tv-energy}
	\Jtv(f) = \normTV{f} = \int \norm{\nabla_x f} \d x.
}
This energy extends to non-smooth functions of bounded variations $f \in \text{BV}([0,1]^2)$. This class contains indicators functions $f=1_{\Om}$ of sets $\Om$ with a bounded perimeter $|\partial \Om|$.

The total variation norm can be computed alternatively using the co-area formula \eqref{eq-tv-coaera}, which shows in particular that $\normTV{1_{\Om}} = |\partial \Om|$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discrete Priors}
\label{subsec-disc-priors}

An analog image $f \in \Ldeux([0,1]^2)$ is discretized through an acquisition device to obtain a discrete image $f \in \RR^N$. Image processing algorithms work on these discrete data, and we thus need to define discrete priors for finite dimensional images. 

%%
\paragraph{Discrete gradient.}

Discrete Sobolev and total variation priors are obtained by computing finite differences approximations of derivatives, using for instance forward differences 
\eqm{
	\de_1 f[n_1,n_2] &= f[n_1+1,n_2]- f[n_1,n_2] \\
	\de_2 f[n_1,n_2] &= f[n_1,n_2+1]- f[n_1,n_2],
}
and one can use higher order schemes to process more precisely smooth functions.
One should be careful with boundary conditions, and we consider here for simplicity periodic boundary conditions, which correspond to computing the indexes $n_i+1$ modulo $N$. More advanced symmetric boundary conditions can be used as well to avoid boundary artifacts. 

A discrete gradient is defined as
\eq{
	\nabla f[n] = ( \de_1 f[n],\de_2 f[n] ) \in \RR^{2}
}
which corresponds to a mapping from images to vector fields
\eq{
	\nabla : \RR^N \longrightarrow \RR^{N \times 2}.
}
Figure \ref{fig-grad-discrete} shows examples of gradient vectors. They point in the direction of the largest slope of the function discretized by $f$. Figure \ref{fig-discrete-operators} shows gradients and their norms displayed as an image. Regions of high gradients correspond to large intensity variations, and thus typically to edges or textures. 

\myfigure{
\tabtrois{
\image{variational}{.3}{grad-discrete}&
\image{variational}{.3}{grad-discrete-zoom-1}&
\image{variational}{.3}{grad-discrete-zoom-2}
}
}{%
	Discrete gradient vectors.
}{fig-grad-discrete}

\paragraph{Discrete divergence.}

One can also use backward differences,
\eqm{
	\tilde\de_1 f[n_1,n_2] &= f[n_1,n_2]- f[n_1-1,n_2] \\
	\tilde\de_2 f[n_1,n_2] &= f[n_1,n_2]- f[n_1,n_2-1].
}
They are dual to the forward differences, so that 
\eq{
	\de_i^* = -\tilde \de_i,
} 
which means that
\eq{
	\foralls f,g \in \RR^N, \quad
	\dotp{\de_i f}{g} = - \dotp{f}{\tilde \de_i g},
}
which is a discrete version of the integration by part formula 
\eq{
	\int_0^1 f' g = - \int_0^1 f g'
}
for smooth periodic functions on $[0,1]$.

A divergence operator is defined using backward differences,
\eq{
	\diverg(v)[n] = \tilde \de_1 v_1[n] + \tilde \de_2 v_2[n],
}
and corresponds to a mapping from vector fields to images
\eq{
	\diverg : \RR^{N \times 2} \longrightarrow \RR^{N}.
}
It is related to the dual of the gradient
\eq{
	\diverg = - \nabla^*
}
which means that
\eq{
	\foralls f \in \RR^N, \;
	\foralls v \in \RR^{N \times 2}, \quad
	\dotp{\nabla f}{v}_{\RR^{N \times 2}} = -\dotp{f}{\diverg(v)}_{\RR^N}
}
which corresponds to a discrete version of the divergence theorem.

\myfigure{
\tabtrois{
\image{variational}{.3}{discrete-operators-image}&
\image{variational}{.3}{discrete-operators-grad}&
\image{variational}{.3}{discrete-operators-gradnorm}\\
Image $f$ & $\nabla f$ & $\norm{\nabla f}$
}
}{%
	Discrete operators.
}{fig-discrete-operators}

%%
\paragraph{Discrete laplacian.}

A general definition of a Laplacian is
\eq{
	\Delta f= \diverg( \nabla f ),
}
which corresponds to a semi-definite negative operator.

For discrete images, and using the previously defined gradient and divergence, it is a local high pass filter
\eql{\label{eq-discrete-lapl}
	\Delta f[n] = \sum_{ p \in V_4(n) } f[p] - 4 f[n],
}
that approximates the continuous second order derivative
\eq{
	\pdd{f}{x_1}(x) + \pdd{f}{x_2} \approx N^2 \De f[n] \qforq x = n/N.
}

Lapalacian operators thus correspond to filterings. A continuous Laplacian is equivalently defined over the Fourier domain in diagonal form as
\eq{
	g = \Delta f \qarrq \hat g(\om) =  \norm{\om}^2 \hat f(\om)
}
and the discrete Laplacian \eqref{eq-discrete-lapl} as
\eql{\label{eq-discrete-lapl-fourier}
	g = \Delta f \qarrq \hat g[\om] =  \rho[\om]^2 \hat f(\om)
	\qwhereq
	\rho[\om]^2 = \sin\pa{ \frac{\pi}{N}\om_1 }^2 + \sin\pa{ \frac{\pi}{N}\om_2 }^2.
}

%%
\paragraph{Discrete energies.}

A discrete Sobolev energy is obtained by using the $\ldeux$ norm of the discrete gradient vector field
\eql{\label{eq-discr-sobolev}
	\Jsob(f) = \frac{1}{2} \sum_n ( \de_1 f[n] )^2 + ( \de_2 f[n] )^2 = \frac{1}{2} \norm{\nabla f}^2.
}
Similarly, a discrete TV energy is defined as the $\lun$ norm of the gradient field
\eql{\label{eq-discr-tv}
	\Jtv(f) = \sum_n \sqrt{ ( \de_1 f[n] )^2 + ( \de_2 f[n] )^2 }
	= \normu{\nabla f}
}
where the $\lun$ norm of a vector field $v \in \RR^{N \times 2}$ is
\eql{\label{eq-lun-norm}
	\normu{v} = \sum_n \norm{v[n]}
}
where $v[n] \in \RR^2$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{PDE and Energy Minimization}

Image smoothing is obtained by minimizing the prior using a gradient descent.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{General Flows}

The gradient of the prior $J : \RR^N \rightarrow \RR$ is a vector $\grad J(f)$. It describes locally up to the first order the variation of the prior
\eq{
	J(f+\epsilon) = J(f) + \dotp{\epsilon}{\grad J(f)} + o(\norm{\epsilon}^2).
}

If $J$ is a smooth function of the image $f$, a discrete energy minimization is obtained through a gradient descent
\eql{\label{eq-gradmin-flow}
	f^{(k+1)} = f^{(k)}  - \tau \grad J(f^{(k)}),
}
where the step size $\tau$ must be small enough to guarantee convergence.

For infinitesimal step size $\tau$, one replaces the discrete parameter $k$ by a continuous time, and the flow
\eq{
	t > 0 \longmapsto f_t \in \RR^N
}
solves the following partial differential equation
\eql{\label{eq-pde-flow}
	\pd{f_t}{t} = - \grad J(f_t)
	\qandq
	f_0 = f.
}
The gradient descent can be seen as an explicit discretization in time of this PDE at times $t_k = k  \tau$.

\myfigure{
\tabquatre{
\image{variational}{.24}{flow-heat-1}&
\image{variational}{.24}{flow-heat-4}&
\image{variational}{.24}{flow-heat-2}&
\image{variational}{.24}{flow-heat-3}\\
\image{variational}{.24}{flow-tv-4}&
\image{variational}{.24}{flow-tv-3}&
\image{variational}{.24}{flow-tv-2}&
\image{variational}{.24}{flow-tv-1}
}
}{%
	Display of $f_t$ for increasing time $t$ for heat flow (top row) and TV flow (bottom row).
}{fig-flow}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Heat Flow}

The heat flow corresponds to the instantiation of the generic PDE \eqref{eq-pde-flow} to the case of the Sobolev energies $\Jsob(f)$ defined for continuous function in \eqref{eq-dfn-sob-energy} and for discrete images in \eqref{eq-discr-sobolev}.

Since it is a quadratic energy, its gradient is easily computed
\eq{
	J(f+\epsilon) = \frac{1}{2}\norm{\nabla f+\nabla \epsilon}^2 = J(f) - \dotp{\Delta f}{\epsilon} + o(\norm{\epsilon}^2),
}
so that 
\eq{
	\grad \Jsob(f) = -\Delta f.
}
Figure \ref{fig-discrete-operators-lapl}, left, shows an example of Laplacian. It is typically large (positive or negative) near edges. 

The heat flow is thus 
\eql{\label{eq-heat-flow-cont}
	\pd{f_t}{t}(x) = - (\grad J(f_t))(x) = \De f_t(x)
	\qandq
	f_0 = f.
}

\myfigure{
\tabdeux{
\image{variational}{.3}{discrete-operators-lapl}&
\image{variational}{.3}{discrete-operators-lapltv}\\
$\diverg(\nabla f)$ & $\diverg(\nabla f / \norm{\nabla f})$ 
}
}{%
	Discrete Laplacian and discrete TV gradient.
}{fig-discrete-operators-lapl}

%%
\paragraph{Continuous in space.}

For continuous images and an unbounded domain $\RR^2$, the PDE \eqref{eq-heat-flow-cont} has an explicit solution as a convolution with a Gaussian kernel of increasing variance as time increases
\eql{\label{eq-heat-gaussian}
	f_t = f \star h_t \qwhereq 
	h_t(x) = \frac{1}{4 \pi t} e^{ -\frac{-\norm{x}^2}{ 4 t } }.
}	
This shows the regularizing property of the heat flow, that operates a blurring to make the image more regular as time evolves.

%%
\paragraph{Discrete in space.}

The discrete Sobolev energy \eqref{eq-discr-sobolev} minimization defined a PDE flow that is discrete in space
\eq{
	\pd{f_t}{t}[n] = - (\grad J(f_t))[n] = \De f_t[n].
}
It can be further discretized in time as done in \eqref{eq-gradmin-flow} and leads to a fully discrete flow
\eq{	
	f^{(k+1)}[n] = f^{(k)}[n] + \tau \Big( \sum_{p \in V_4(n)} f[p] - 4 f[n] \Big) = f \star h [n]
}
where $V_4(n)$ are the four neighbor to a pixel $n$. The flow thus corresponds to iterative convolutions
\eq{
	f^{(k)} = f \star h \star \ldots \star h  = f \star^k h.
}
where $h$ is a discrete filter.

It can be shown to be stable and convergent if $\tau<1/4$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Total Variation Flows}

%%
\paragraph{Total variation gradient.}

The total variation energy $\Jtv$, both continuous \eqref{eq-dfn-tv-energy} and discrete \eqref{eq-discr-tv} is not a smooth function of the image. For instance, the discrete $\Jtv$ is non-differentiable at an image $f$ such that there exists a pixel $n$ where $\nabla f[n] = 0$.

If $\nabla f[n] \neq 0$, one can compute the gradient of the TV energy specialized at pixel $n$ as
\eq{
	(\grad J(f))[n] = -\diverg\pa{ \frac{ \nabla f}{ \norm{\nabla f} } }[n]
}
which exhibits a division by zero singularity for a point with vanishing gradient. Figure \ref{fig-discrete-operators-lapl} shows an example of TV gradient, which appears noisy in smooth areas, because $\norm{\nabla f[n]}$ is small in such regions.

This non-differentiabilty makes impossible the definition of a gradient descent and a TV flow.

%%
\paragraph{Regularized total variation.}
	
To avoid this issue, one can modify the TV energy, and define a smoothed TV prior
\eql{\label{eq-tv-smoothed}
	\Jtv^\epsilon(f) = \sum_n \sqrt{ \epsilon^2 + \norm{ \nabla f[n] }^2 }
}
where $\epsilon>0$ is a small regularization parameter. Figure \ref{fig-regul-abs} shows this effect of this regularization on the absolute value function.


\myfigure{
\tabdeux{
\image{variational}{.3}{regul-abs-1}&
\image{variational}{.3}{regul-abs-2}\\
$\epsilon=0.1$ & $\epsilon=0.01$
}
}{%
	Regularized absolute value $x \mapsto \sqrt{x^2+\epsilon^2}$.
}{fig-regul-abs}

This smoothed TV energy is a differentiable function of the image, and its gradient is
\eql{\label{eq-grad-tv-eps}
	\grad \Jtv^\epsilon(f) = -\diverg\pa{ \frac{ \nabla f}{ \sqrt{\epsilon^2+\norm{\nabla f}^2} } }.
}
One can see that this smoothing interpolate between TV and Sobolev, as
\eq{
	\grad_f^\epsilon \sim  -\Delta / \epsilon \quad\text{when}\quad \epsilon \rightarrow +\infty.
}
Figure \ref{fig-discrete-operators-tveps} shows the evolution of this gradient for several value of the smoothing parameter.

\myfigure{
\tabquatre{
\image{variational}{.24}{discrete-operators-tveps-1}&
\image{variational}{.24}{discrete-operators-tveps-2}&
\image{variational}{.24}{discrete-operators-tveps-3}&
\image{variational}{.24}{discrete-operators-tveps-4}\\
$\epsilon=0.01$ & $\epsilon=0.05$ & $\epsilon=0.1$ & $\epsilon=0.5$
}
}{%
	Regularized gradient norm $\sqrt{ \norm{\nabla f(x)}^2 + \epsilon^2 }$.
}{fig-discrete-operators-tveps}


%%
\paragraph{Regularized total variation flow.}

The smoothed total variation flow is then defined as
\eql{\label{eq-tv-eps-flow}
	\pd{f_t}{t} = \diverg\pa{ \frac{ \nabla f_t}{ \sqrt{\epsilon^2+\norm{\nabla f_t}^2} } }.
}
Choosing a small $\epsilon$ makes the flow closer to a minimization of the total variation, but makes the computation unstable.

In practice, the flow is computed with a discrete gradient descent \eqref{eq-gradmin-flow}. For the smoothed total variation flow to converge, one needs to impose that $\tau < \epsilon/4$, which shows that being more faithful to the TV energy requires smaller time steps and thus slower algorithms.

Figure \ref{fig-flow} shows a comparison between the heat flow and the total variation flow for a small value of $\epsilon$. This shows that the TV flow smooth less the edges than heat diffusion, which is consistent with the ability of the TV energy to better characterize sharp edges.

% Table \ref{wavelets-variational-tv} details the implementation of this smoothed TV flow.

%\matlab{matlab/variational-tv.m
% }{Smoothed total variation flow, computed by gradient descent. The input is the image \matvar{f}, the output is \matvar{ftv},  and approximation of the solution $f_{T}$ of the smoothed TV flow \eqref{eq-tv-eps-flow} at time $t=T$. The regularization parameter $\epsilon$ is stored in \matvar{epsilon}.
% }{wavelets-variational-tv}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{PDE Flows for Denoising}
\label{subsec-pde-denoise}

PDE flows can be used to remove noise from an observation $f= f_0 + w$. As detailed in Section \ref{subsec-image-formation} a simple noise model assumes that each pixel is corrupted with a Gaussian noise $w[n] \sim \Nn(0,\si)$, and that these perturbations are independent (white noise).

The denoising is obtained using the PDE flow within initial image $f$ at time $t=0$
\eq{
	\pd{f_t}{t} = - \grad_{f_t} J
	\qandq
	f_{t=0} = f.
}
An estimator $\tilde f = f_{t_0}$ is obtained for a well chose $t=t_0$. Figure \ref{fig-denoising-flow} shows examples of Sobolev and TV flows for denoising.

\myfigure{
\tabquatre{
\image{variational}{.24}{denoising-flow-sob-1}&
\image{variational}{.24}{denoising-flow-sob-2}&
\image{variational}{.24}{denoising-flow-sob-3}&
\image{variational}{.24}{denoising-flow-sob-4}\\
\image{variational}{.24}{denoising-flow-tv-1}&
\image{variational}{.24}{denoising-flow-tv-2}&
\image{variational}{.24}{denoising-flow-tv-3}&
\image{variational}{.24}{denoising-flow-tv-4}
}
}{%
	Denoising using $f_t$ displayed for various time $t$ for Sobolev (top) and TV (bottom) flows.
}{fig-denoising-flow}

Since  $f_t$ converges to a constant image when $t \rightarrow +\infty$, the choice of $t_0$ corresponds to a tradeoff between removing enough noise and not smoothing too much the edges in the image. This is usually a difficult task. During simulation, if one has access to the clean image $f_0$, one can monitor the denoising error $\norm{f_0-f_t}$ and choose the $t=t_0$ that minimizes this error. Figure \ref{fig-denoising-cameraman}, top row, shows an example of this oracle estimation of the best stopping time.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Regularization for Denoising}

Instead of performing a gradient descent flow for denoising as detailed in Section \ref{subsec-pde-denoise} and select a stopping time, one can formulate an optimization problem. The estimator is then defined as a solution of this optimization. This setup has the advantage as being well defined mathematically even for non-smooth priors such as the TV prior $\Jtv$ or the sparsity prior $\Ju$. Furthermore, this regularization framework is also useful to solve general inverse problems as detailed in Chapter \ref{chap-invpbm}. 


\myfigure{
\tabtrois{
\image{variational}{.32}{denoising-cameraman-noisy}&
\image{variational}{.32}{denoising-cameraman-heat}&
\image{variational}{.32}{denoising-cameraman-tvflow}\\
Noisy $f$ & Sobolev flow & TV flow \\
\image{variational}{.32}{denoising-cameraman-sobreg}&
\image{variational}{.32}{denoising-cameraman-tvreg}&
\image{variational}{.32}{denoising-cameraman-wavti}\\
Sobolev reg & TV reg & TI wavelets
}
}{%
	Denoising using PDE flows and regularization.
}{fig-denoising-cameraman}

\myfigure{
\tabdeux{
\image{variational}{.4}{denoising-cameraman-heat-snr}&
\image{variational}{.4}{denoising-cameraman-tvflow-snr}\\
Sobolev flow & TV flow \\
\image{variational}{.4}{denoising-cameraman-tvreg-snr}&
\image{variational}{.4}{denoising-cameraman-sobreg-snr}\\
Sobolev regularization & TV regularization
}
}{%
	SNR as a function of time $t$ for flows (top) and $\la$ for regularization (bottom).
}{fig-denoising-cameraman-snr}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Regularization}

Given some noisy image $f= f_0 + w$ of $N$ pixels and a prior $J$, we consider the convex optimization problem
\eql{\label{eq-regul-denoising}
	f^\star_\la \in \uargmin{ g \in \RR^N } \frac{1}{2} \norm{f-g}^2 + \la J(g),
}
where $\la > 0$ is a Lagrange multiplier parameter that weights the influence of the data fitting term $\norm{f-g}^2$ and the regularization term $J(g)$. 

If one has at his disposal a clean original image $f_0$, one can minimize the denoising error $\norm{f^\star_\la-f_0}$, but it is rarely the case. In practice, this parameter should be adapted to the noise level and to the regularity of the unknown image $f_0$, which might be a non trivial task. 

We note that since we did not impose $J$ to be convex, the problem \eqref{eq-regul-denoising} might have several optimal solutions. 

An estimator is thus defined as 
\eq{
	\tilde f = f^\star_\la
} 
for a well chosen $\la$.

If $J$ is differentiable and convex, one can compute the solution of \eqref{eq-regul-denoising} through a gradient descent
\eql{\label{eq-grad-desc-regul}
	f^{(k+1)} = f^{(k)} - \tau \pa{ f^{(k)} -\la \grad J(f^{(k)} }
}
where the descent step size $\tau>0$ should be small enough. This gradient descent is similar to the time-discretized minimization flow \eqref{eq-gradmin-flow}, excepted that the data fitting term prevent the flow to converge to a constant image. 

Unfortunately, priors such as the total variation $\Jtv$ or the sparsity $\Ju$ are non-differentiable. Some priors such as the ideal sparsity $\Jz$ might even be non-convex. This makes the simple gradient descent not usable to solve for \eqref{eq-regul-denoising}. In the following Section we show how to compute $f^\star_\la$ for several priors.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sobolev Regularization}

The discrete Sobolev prior defined in \eqref{eq-discr-sobolev} is differentiable, and the gradient descent \eqref{eq-grad-desc-regul} reads 
\eq{
	f^{(k+1)} =  (1-\tau) f^{(k)} + \tau f - \tau \la \Delta J(f^{(k)}.
}
Since $J(f)=\norm{\nabla f}^2$ is quadratic, one can use a conjugate gradient descent, which converges faster. 

The solution $f^\star_\la$ can be computed in closed form as the solution of a linear system
\eq{
	f^\star_\la = ( \Id_{N} - \la \Delta )^{-1} f,
}
which shows that the regularization \eqref{eq-regul-denoising} is computing an estimator that depends linearly on the observations $f$.

If the differential operators are computed with periodic boundary conditions, this linear system can be solved exactly over the Fourier domain
\eql{\label{eq-sobol-reg-fourier}
	\hat f^\star_\la[\om] = \frac{1}{1 + \la \rho_\om^2 } \hat f[\om]
}
where $\rho_\om$ depends on the discretization of the Laplacian, see for instance \eqref{eq-discrete-lapl-fourier}.

Equation \eqref{eq-sobol-reg-fourier} shows that denoising using Sobolev regularization corresponds to a low pass filtering, whose strength is controlled by $\la$. This should be related to the solution \eqref{eq-heat-gaussian} of the heat equation, which also corresponds to a filtering, but using a Gaussian low pass kernel parameterized by its variance $t^2$.

This Sobolev regularization denoising is a particular case of the linear estimator considered in Section \ref{sec-linear-denoising}. The selection of the parameter $\la$ is related to the selection of an optimal filter as considered in Section \ref{subsec-optimal-selection-filter}, but with the restriction that the filter is computed in a parametric family.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{TV Regularization}

The total variation prior $\Jtv$ defined in \eqref{eq-discr-tv} is non-differentiable. One can either use a smoothed approximation of the prior, or use an optimization algorithm not based on a gradient descent.

%%
\paragraph{Smoothed TV.}

The TV prior can be approximated to obtain the prior $\Jtv^\epsilon(g)$ defined in \eqref{eq-tv-smoothed}, which is differentiable with respect to $g$.
Using the gradient of this prior defined in \eqref{eq-grad-tv-eps}, one obtains the following instantiation of the gradient descent \eqref{eq-grad-desc-regul} 
\eql{\label{eq-grad-tveps-regul}
	f^{(k+1)} = (1-\tau) f^{(k)} + \tau f + 
		\la\tau \diverg\pa{ \frac{ \nabla f_t}{ \sqrt{\epsilon^2+\norm{\nabla f_t}^2} } }.
}
which converge to the unique minimizer $f^\star_\la$ of \eqref{eq-regul-denoising}.

%%
\paragraph{Chambolle's algorithm.}

Chambolle in \cite{chambolle-algo-tv} detail an algorithm to minimize exactly the TV denoising problem
\eql{\label{eq-prox-tv}
	f_\la^\star = \uargmin{g \in \RR^N} \frac{1}{2} \norm{f-g}^2 + \la \normTV{g}.
}
It uses a relationship between the vectorial $\lun$ and $\linf$ norms
\eq{
	\normu{v} = \sum_{m=0}^{N-1} \norm{v[m]} \qandq
	\normi{v} = \umax{0 \leq m < N} \norm{v[m]}
}
where each $v[m] \in \RR^2$ and $v \in \RR^{N \times 2}$. One has
\eq{
	\normu{v} = \umax{ \normi{w} \leq 1 } \dotp{w}{u}
}
which allows one to re-write the optimization \eqref{eq-prox-tv} as
\eq{
	\umin{g \in \RR^N} \umax{\normi{w} \leq 1} 
		\frac{1}{2} \norm{f-g}^2 + \la \dotp{w}{\nabla g}.
}
Exchanging the roles of the min and the max, one proves that the solution of  \eqref{eq-prox-tv} is re-written as
\eql{\label{eq-primal-dual-chambolle}
	f_\la^\star =  f + \la \diverg(w^\star)
}
where 
\eql{\label{eq-chambolle-dual}
	w^\star \in \uargmin{\normi{w} \leq 1} \norm{f + \la \diverg(w^\star)}^2.
}
The convex optimization problem \eqref{eq-chambolle-dual} computes a dual vector field $w^\star \in \RR^{N \times 2}$, from which the denoised image is recovered using \eqref{eq-primal-dual-chambolle}.

The dual problem \eqref{eq-chambolle-dual} is the minimization of a quadratic functional subject to a convex $\linf$ constraint. It can thus be solved using for instance a projected gradient descent
\eq{
	w^{(k+1)}[m] = \frac{ \tilde w^{(k)}[m] }{ \max(|\tilde w^{(k)}[m]|,1) }
	\qwhereq
	\tilde w^{(k)} = w^{(k)} + \tau \nabla( f/\la + \diverg(w^{(k)}) ).
}  
If the gradient step size satisfy $0 < \tau < 1/4$, one can prove that 
\eq{
	f + \la \diverg(w^{(k)}) \longrightarrow f_\la^\star
	\qwhenq
	k \rightarrow +\infty.
}


%\matlab{matlab/variational-tv-chambolle.m
%}{Total variation regularization computed using Chambolle's algorithm. The input is the noisy image \matvar{f}, the output is \matvar{ftv}, the solution $f_\la$ of \eqref{eq-prox-tv}.
%The regularization parameter $\la$ is stored in \matvar{lambda}.
%}{wavelets-variational-tv-chambolle}
