\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Motivation in Machine Learning}{2}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Unconstraint optimization}{2}{subsection.1.1}}
\newlabel{eq-general-pbm}{{1}{2}{Unconstraint optimization}{equation.1.1}{}}
\newlabel{eq-general-pbm-min}{{2}{2}{Unconstraint optimization}{equation.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Left: linear regression, middle: linear classifier, right: loss function for classification. }}{3}{figure.1}}
\newlabel{fig-ml-ex}{{1}{3}{Left: linear regression, middle: linear classifier, right: loss function for classification}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Left: non-existence of minimizer, middle: multiple minimizers, right: uniqueness. }}{3}{figure.2}}
\newlabel{fig-minimizer-exists}{{2}{3}{Left: non-existence of minimizer, middle: multiple minimizers, right: uniqueness}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Regression}{3}{subsection.1.2}}
\newlabel{eq-least-square}{{3}{3}{Regression}{equation.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Classification}{3}{subsection.1.3}}
\newlabel{eq-classif}{{4}{3}{Classification}{equation.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Basics of Convex Analysis}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Existence of Solutions}{3}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Coercivity condition for least squares. }}{4}{figure.3}}
\newlabel{fig-least-square}{{3}{4}{Coercivity condition for least squares}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Convex vs. non-convex functions ; Strictly convex vs. non strictly convex functions. }}{4}{figure.4}}
\newlabel{fig-cvx-vs-noncvx}{{4}{4}{Convex vs. non-convex functions ; Strictly convex vs. non strictly convex functions}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Convexity}{4}{subsection.2.2}}
\newlabel{eq-convexity-def}{{5}{4}{Convexity}{equation.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Comparison of convex functions $f : \mathbb  {R}^p \rightarrow \mathbb  {R}$ (for $p=1$) and convex sets $C \subset \mathbb  {R}^p$ (for $p=2$). }}{5}{figure.5}}
\newlabel{fig-cvx-set}{{5}{5}{Comparison of convex functions $f : \RR ^p \rightarrow \RR $ (for $p=1$) and convex sets $C \subset \RR ^p$ (for $p=2$)}{figure.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Strict convexity.}{5}{section*.2}}
\newlabel{eq-strict-convexity-def}{{6}{5}{Strict convexity}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Convex Sets}{5}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Derivative and gradient}{5}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient}{5}{subsection.3.1}}
\newlabel{eq-grad-dfn}{{7}{6}{Gradient}{equation.3.7}{}}
\newlabel{prop-above-tgt}{{1}{6}{}{prop.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}First Order Conditions}{6}{subsection.3.2}}
\newlabel{prop-cs-min}{{2}{6}{}{prop.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Function with local maxima/minima (left), saddle point (middle) and global minimum (right). }}{7}{figure.6}}
\newlabel{fig-first-order}{{6}{7}{Function with local maxima/minima (left), saddle point (middle) and global minimum (right)}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Least Squares}{7}{subsection.3.3}}
\newlabel{eq-grad-ls}{{8}{8}{Least Squares}{equation.3.8}{}}
\newlabel{eq-sol-leastsquare}{{9}{8}{Least Squares}{equation.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Link with PCA}{8}{subsection.3.4}}
\newlabel{eq-pca-decomp}{{10}{8}{Link with PCA}{equation.3.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Left: point clouds $(a_i)_i$ with associated PCA directions, right: quadratic part of $f(x)$. }}{9}{figure.7}}
\newlabel{fig-link-pca}{{7}{9}{Left: point clouds $(a_i)_i$ with associated PCA directions, right: quadratic part of $f(x)$}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Classification}{9}{subsection.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Chain Rule}{9}{subsection.3.6}}
\newlabel{eq-grad-composition-linear}{{11}{9}{Chain Rule}{equation.3.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Left: First order Taylor expansion in 1-D and 2-D. Right: orthogonality of gradient and level sets and schematic of the proof. }}{10}{figure.8}}
\newlabel{fig-expansion-taylor}{{8}{10}{Left: First order Taylor expansion in 1-D and 2-D. Right: orthogonality of gradient and level sets and schematic of the proof}{figure.8}{}}
\newlabel{eq-differential-defn}{{12}{10}{Chain Rule}{equation.3.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Gradient Descent Algorithm}{10}{section.4}}
\newlabel{sec-grad-desc-basic}{{4}{10}{Gradient Descent Algorithm}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Steepest Descent Direction}{10}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Gradient Descent}{11}{subsection.4.2}}
\newlabel{eq-grad-desc}{{13}{11}{Gradient Descent}{equation.4.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  Influence of $\tau $ on the gradient descent (left) and optimal step size choice (right). }}{12}{figure.9}}
\newlabel{fig-gradesc}{{9}{12}{Influence of $\tau $ on the gradient descent (left) and optimal step size choice (right)}{figure.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Convergence Analysis}{12}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Quadratic Case}{12}{subsection.5.1}}
\@writefile{toc}{\contentsline {paragraph}{Convergence analysis for the quadratic case.}{12}{section*.3}}
\newlabel{prop-graddesc-quad}{{4}{12}{}{prop.4}{}}
\newlabel{eq-global-linrate-grad}{{14}{12}{}{equation.5.14}{}}
\newlabel{eq-best-rate-local}{{15}{12}{}{equation.5.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  Contraction constant $h(\tau )$ for a quadratic function (right). }}{13}{figure.10}}
\newlabel{fig-grad-desc-contract}{{10}{13}{Contraction constant $h(\tau )$ for a quadratic function (right)}{figure.10}{}}
\newlabel{eq-rate-strong-quad}{{16}{13}{Convergence analysis for the quadratic case}{equation.5.16}{}}
\newlabel{prop-graddesc-quad-sublin}{{5}{14}{}{prop.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}General Case}{15}{subsection.5.2}}
\@writefile{toc}{\contentsline {paragraph}{Hessian.}{15}{section*.4}}
\newlabel{eq-taylor-hess}{{17}{15}{Hessian}{equation.5.17}{}}
\@writefile{toc}{\contentsline {paragraph}{Smoothness and strong convexity.}{16}{section*.5}}
\newlabel{eq-lipsch-grad}{{{$\mathcal  {R}_L$}}{16}{Smoothness and strong convexity}{equation.5.18}{}}
\newlabel{eq-strong-conv}{{{$\mathcal  {S}_\mu $}}{16}{Smoothness and strong convexity}{equation.5.18}{}}
\newlabel{prop-smooth-strong}{{6}{16}{}{prop.6}{}}
\newlabel{eq-above-below-quad}{{18}{16}{}{equation.5.18}{}}
\newlabel{eq-upper-lower-bound-hess}{{19}{16}{}{equation.5.19}{}}
\@writefile{toc}{\contentsline {paragraph}{Convergence analysis.}{17}{section*.6}}
\newlabel{thm-gradsec-non-strong-conv}{{1}{17}{}{thm.1}{}}
\newlabel{eq-sublin-rate-gd}{{20}{17}{}{equation.5.20}{}}
\newlabel{eq-proox-x'rad-nonstrong-1}{{21}{17}{Convergence analysis}{equation.5.21}{}}
\newlabel{eq-conv-rate-proof-1}{{24}{17}{Convergence analysis}{equation.5.24}{}}
\newlabel{eq-rate-strong}{{25}{18}{Convergence analysis}{equation.5.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Regularization}{18}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Penalized Least Squares}{18}{subsection.6.1}}
\newlabel{eq-regul-ls}{{26}{18}{Penalized Least Squares}{equation.6.26}{}}
\newlabel{eq-regul-constr}{{27}{18}{}{equation.6.27}{}}
\newlabel{eq-ineq-proof-regul}{{28}{18}{Penalized Least Squares}{equation.6.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Ridge Regression}{19}{subsection.6.2}}
\newlabel{eq-regul-ls-1}{{29}{19}{}{equation.6.29}{}}
\newlabel{eq-regul-ls-2}{{30}{19}{}{equation.6.30}{}}
\@writefile{toc}{\contentsline {paragraph}{Pseudo-inverse.}{19}{section*.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Lasso}{19}{subsection.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces $\ell ^q$ balls $ \left \{ x \tmspace  +\thickmuskip {.2777em};\tmspace  +\thickmuskip {.2777em} \DOTSB \sum@ \slimits@ _k |x_k|^q \leqslant 1 \right \} $ for varying $q$. }}{20}{figure.11}}
\newlabel{fig-sparsity-lq}{{11}{20}{$\ell ^q$ balls $\enscond {x}{\sum _k |x_k|^q \leq 1}$ for varying $q$}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Evolution with $\lambda $ of the function $F(x) \ensuremath  {\mathrel {\mathop {=}\limits ^{\unhbox \voidb@x \hbox {\upshape  \relax \fontsize  {5}{6}\selectfont  def.}}}}\frac  {1}{2}|\tmspace  -\thinmuskip {.1667em}| \cdot -y |\tmspace  -\thinmuskip {.1667em}|^2+\lambda |\cdot |$. }}{20}{figure.12}}
\newlabel{fig-varspars}{{12}{20}{Evolution with $\la $ of the function $F(x) \eqdef \frac {1}{2}\norm {\cdot -y}^2+\la |\cdot |$}{figure.12}{}}
\newlabel{prop-soft-tresdh}{{9}{20}{}{prop.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Iterative Soft Thresholding}{21}{subsection.6.4}}
\newlabel{sec-ista}{{6.4}{21}{Iterative Soft Thresholding}{subsection.6.4}{}}
\newlabel{eq-ista-surrog}{{31}{21}{Iterative Soft Thresholding}{equation.6.31}{}}
\newlabel{eq-ista}{{32}{21}{}{equation.6.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Stochastic Optimization}{21}{section.7}}
\newlabel{sec-stochastic-optim}{{7}{21}{Stochastic Optimization}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Minimizing Sums and Expectation}{22}{subsection.7.1}}
\newlabel{eq-min-sums}{{33}{22}{Minimizing Sums and Expectation}{equation.7.33}{}}
\newlabel{eq-min-int}{{34}{22}{Minimizing Sums and Expectation}{equation.7.34}{}}
\newlabel{eq-stochastic-erm}{{35}{22}{Minimizing Sums and Expectation}{equation.7.35}{}}
\newlabel{eq-stoch-logistic}{{36}{22}{Minimizing Sums and Expectation}{equation.7.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Batch Gradient Descent (BGD)}{22}{subsection.7.2}}
\newlabel{eq-full-grad}{{37}{22}{Batch Gradient Descent (BGD)}{equation.7.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  Evolution of the error of the BGD for logistic classification. }}{23}{figure.13}}
\newlabel{fig-bgd}{{13}{23}{Evolution of the error of the BGD for logistic classification}{figure.13}{}}
\newlabel{eq-grad-formula}{{38}{23}{Batch Gradient Descent (BGD)}{equation.7.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Stochastic Gradient Descent (SGD)}{23}{subsection.7.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Unbiased gradient estimate}}{23}{figure.14}}
\newlabel{eq-unbiased-grad}{{39}{23}{Stochastic Gradient Descent (SGD)}{equation.7.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Schematic view of SGD iterates}}{23}{figure.15}}
\newlabel{eq-stepsize-sgd}{{40}{23}{Stochastic Gradient Descent (SGD)}{equation.7.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  Display of a large number of trajectories $k \DOTSB \mapstochar \rightarrow x_k \in \mathbb  {R}$ generated by several runs of SGD. On the top row, each curve is a trajectory, and the bottom row displays the corresponding density. }}{24}{figure.16}}
\newlabel{fig-sgd-traject}{{16}{24}{Display of a large number of trajectories $k \mapsto x_k \in \RR $ generated by several runs of SGD. On the top row, each curve is a trajectory, and the bottom row displays the corresponding density}{figure.16}{}}
\newlabel{thm-conv-sgd}{{2}{24}{}{thm.2}{}}
\newlabel{eq-rate-sgd}{{41}{24}{}{equation.7.41}{}}
\newlabel{eq-sgd-proof-1}{{42}{24}{Stochastic Gradient Descent (SGD)}{equation.7.42}{}}
\newlabel{eq-sgd-proof-2}{{43}{24}{Stochastic Gradient Descent (SGD)}{equation.7.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  Evolution of the error of the SGD for logistic classification (dashed line shows BGD). }}{25}{figure.17}}
\newlabel{fig-sgd}{{17}{25}{Evolution of the error of the SGD for logistic classification (dashed line shows BGD)}{figure.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Stochastic Gradient Descent with Averaging (SGA)}{25}{subsection.7.4}}
\newlabel{sec-sga}{{7.4}{25}{Stochastic Gradient Descent with Averaging (SGA)}{subsection.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces  Evolution of $\qopname  \relax o{log}_{10}(f(x_k)-f(x^\star ))$ for SGD, SGA and SAG. }}{26}{figure.18}}
\newlabel{fig-compariso-sgd}{{18}{26}{Evolution of $\log _{10}(f(x_k)-f(x^\star ))$ for SGD, SGA and SAG}{figure.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Stochastic Averaged Gradient Descent (SAG)}{26}{subsection.7.5}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Multi-Layers Perceptron}{26}{section.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}MLP and its derivative}{26}{subsection.8.1}}
\@writefile{toc}{\contentsline {paragraph}{Expressiveness. }{27}{section*.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}MLP and Gradient Computation}{27}{subsection.8.2}}
\@writefile{toc}{\contentsline {paragraph}{Optimizing with respect to $u$.}{27}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{Optimizing with respect to $W$.}{28}{section*.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Universality}{28}{subsection.8.3}}
\newlabel{sec-universality}{{8.3}{28}{Universality}{subsection.8.3}{}}
\newlabel{eq-constraint-univ}{{44}{28}{Universality}{equation.8.44}{}}
\newlabel{thm-universality}{{3}{28}{Cybenko, 1989}{thm.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Proof in dimension $p=1$.}{29}{section*.11}}
\@writefile{toc}{\contentsline {paragraph}{Proof in arbitrary dimension $p$.}{29}{section*.12}}
\newlabel{prop-proof-univ-1}{{11}{29}{}{prop.11}{}}
\newlabel{eq-prop-proof-univ-1}{{45}{29}{}{equation.8.45}{}}
\@writefile{toc}{\contentsline {paragraph}{Quantitative rates.}{30}{section*.13}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Automatic Differentiation}{30}{section.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  A computational graph. }}{31}{figure.19}}
\newlabel{fig-compgraph}{{19}{31}{A computational graph}{figure.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Computational Graphs}{31}{subsection.9.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Forward Mode of Automatic Differentiation}{31}{subsection.9.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces  Example of a simple computational graph. }}{32}{figure.20}}
\newlabel{fig-dag-example-simple}{{20}{32}{Example of a simple computational graph}{figure.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces  Example of a more complex computational graph. }}{32}{figure.21}}
\newlabel{fig-dag-example-complex}{{21}{32}{Example of a more complex computational graph}{figure.21}{}}
\@writefile{toc}{\contentsline {paragraph}{Simple example.}{32}{section*.14}}
\newlabel{eq-simple-func-autodiff}{{46}{32}{Simple example}{equation.9.46}{}}
\@writefile{toc}{\contentsline {paragraph}{More complex example.}{32}{section*.15}}
\newlabel{eq-cpx-func-autodiff}{{47}{32}{More complex example}{equation.9.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Reverse Mode of Automatic Differentiation}{33}{subsection.9.3}}
\@writefile{toc}{\contentsline {paragraph}{Back-propagation.}{33}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{Simple example.}{34}{section*.17}}
\@writefile{toc}{\contentsline {paragraph}{More complex example.}{34}{section*.18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Feed-forward Architectures}{34}{subsection.9.4}}
\newlabel{eq-simple-lin-dag}{{48}{34}{Feed-forward Architectures}{equation.9.48}{}}
\bibstyle{plain}
\bibdata{all}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces  Complexity of forward (left) and backward (right) modes for feedforward graphs. }}{35}{figure.22}}
\newlabel{fig-matrix-mult}{{22}{35}{Complexity of forward (left) and backward (right) modes for feedforward graphs}{figure.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Multilayer Perceptron}{35}{subsection.9.5}}
\newlabel{sec-autodiff-mlp}{{9.5}{35}{Multilayer Perceptron}{subsection.9.5}{}}
\newlabel{eq-mlp-def}{{49}{35}{Multilayer Perceptron}{equation.9.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces  Computational graph for a MLP. }}{36}{figure.23}}
\newlabel{fig-mlp}{{23}{36}{Computational graph for a MLP}{figure.23}{}}
