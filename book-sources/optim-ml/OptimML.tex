\documentclass[10pt]{article}

% Be sure to use PDF Latex
\pdfoutput=1

% links
\usepackage[bookmarks,bookmarksdepth=2, colorlinks=true, linkcolor=blue,citecolor=red, urlcolor=blue]{hyperref}


\usepackage{fullpage}

\usepackage[latin1]{inputenc}
\usepackage{../mystyle}
\usepackage{wrapfig}


\newcommand{\dims}{d}


\graphicspath{{../figures/}}



\title{Course notes on\\Optimization for Machine Learning} 

\author{%
\begin{tabular}{c}
	Gabriel Peyr{\'e} \\ CNRS \& DMA \\
	 \'Ecole Normale Sup\'erieure \\
	 \url{gabriel.peyre@ens.fr}\\
	 \url{https://mathematical-tours.github.io}\\
	 \url{www.numerical-tours.com}
\end{tabular}
}


\date{\today}

%%

\begin{document}

\maketitle

\begin{abstract}
		This document presents first order optimization methods and their applications to machine learning. 
		%
		This is not a course on machine learning (in particular it does not cover modeling and statistical considerations) and it is focussed on the use and analysis of cheap methods that can scale to large datasets and models with lots of parameters. These methods are variations around the notion of ``gradient descent'', so that the computation of gradients plays a major role.
		%
		This course covers basic theoretical properties of optimization problems (in particular convex analysis and first order differential calculus), the gradient descent method, the stochastic gradient method, automatic differentiation, shallow and deep networks.  
\end{abstract}

\tableofcontents

 

\input{../chapters/sec-optim-smooth}
\input{../chapters/sec-regul}
\input{../chapters/sec-stochastic-optim}
\input{../chapters/sec-mlp}

\section{Automatic Differentiation}
\input{../chapters/sec-autodiff}


% \input{sections/matching}



% \nocite{*}

\bibliographystyle{plain}
\bibliography{all}

\end{document}
