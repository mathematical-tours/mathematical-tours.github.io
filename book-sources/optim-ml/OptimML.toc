\contentsline {section}{\numberline {1}Motivation in Machine Learning}{2}{section.1}
\contentsline {subsection}{\numberline {1.1}Unconstraint optimization}{2}{subsection.1.1}
\contentsline {subsection}{\numberline {1.2}Regression}{3}{subsection.1.2}
\contentsline {subsection}{\numberline {1.3}Classification}{3}{subsection.1.3}
\contentsline {section}{\numberline {2}Basics of Convex Analysis}{3}{section.2}
\contentsline {subsection}{\numberline {2.1}Existence of Solutions}{3}{subsection.2.1}
\contentsline {subsection}{\numberline {2.2}Convexity}{4}{subsection.2.2}
\contentsline {paragraph}{Strict convexity.}{5}{section*.2}
\contentsline {subsection}{\numberline {2.3}Convex Sets}{5}{subsection.2.3}
\contentsline {section}{\numberline {3}Derivative and gradient}{5}{section.3}
\contentsline {subsection}{\numberline {3.1}Gradient}{5}{subsection.3.1}
\contentsline {subsection}{\numberline {3.2}First Order Conditions}{6}{subsection.3.2}
\contentsline {subsection}{\numberline {3.3}Least Squares}{7}{subsection.3.3}
\contentsline {subsection}{\numberline {3.4}Link with PCA}{8}{subsection.3.4}
\contentsline {subsection}{\numberline {3.5}Classification}{9}{subsection.3.5}
\contentsline {subsection}{\numberline {3.6}Chain Rule}{9}{subsection.3.6}
\contentsline {section}{\numberline {4}Gradient Descent Algorithm}{10}{section.4}
\contentsline {subsection}{\numberline {4.1}Steepest Descent Direction}{10}{subsection.4.1}
\contentsline {subsection}{\numberline {4.2}Gradient Descent}{11}{subsection.4.2}
\contentsline {section}{\numberline {5}Convergence Analysis}{12}{section.5}
\contentsline {subsection}{\numberline {5.1}Quadratic Case}{12}{subsection.5.1}
\contentsline {paragraph}{Convergence analysis for the quadratic case.}{12}{section*.3}
\contentsline {subsection}{\numberline {5.2}General Case}{15}{subsection.5.2}
\contentsline {paragraph}{Hessian.}{15}{section*.4}
\contentsline {paragraph}{Smoothness and strong convexity.}{16}{section*.5}
\contentsline {paragraph}{Convergence analysis.}{17}{section*.6}
\contentsline {subsection}{\numberline {5.3}Acceleration}{18}{subsection.5.3}
\contentsline {section}{\numberline {6}Regularization}{19}{section.6}
\contentsline {subsection}{\numberline {6.1}Penalized Least Squares}{19}{subsection.6.1}
\contentsline {subsection}{\numberline {6.2}Ridge Regression}{20}{subsection.6.2}
\contentsline {paragraph}{Pseudo-inverse.}{20}{section*.7}
\contentsline {subsection}{\numberline {6.3}Lasso}{21}{subsection.6.3}
\contentsline {subsection}{\numberline {6.4}Iterative Soft Thresholding}{22}{subsection.6.4}
\contentsline {section}{\numberline {7}Stochastic Optimization}{23}{section.7}
\contentsline {subsection}{\numberline {7.1}Minimizing Sums and Expectation}{23}{subsection.7.1}
\contentsline {subsection}{\numberline {7.2}Batch Gradient Descent (BGD)}{23}{subsection.7.2}
\contentsline {subsection}{\numberline {7.3}Stochastic Gradient Descent (SGD)}{24}{subsection.7.3}
\contentsline {subsection}{\numberline {7.4}Stochastic Gradient Descent with Averaging (SGA)}{26}{subsection.7.4}
\contentsline {subsection}{\numberline {7.5}Stochastic Averaged Gradient Descent (SAG)}{27}{subsection.7.5}
\contentsline {section}{\numberline {8}Multi-Layers Perceptron}{27}{section.8}
\contentsline {subsection}{\numberline {8.1}MLP and its derivative}{28}{subsection.8.1}
\contentsline {paragraph}{Expressiveness. }{28}{section*.8}
\contentsline {subsection}{\numberline {8.2}MLP and Gradient Computation}{28}{subsection.8.2}
\contentsline {paragraph}{Optimizing with respect to $u$.}{29}{section*.9}
\contentsline {paragraph}{Optimizing with respect to $W$.}{29}{section*.10}
\contentsline {subsection}{\numberline {8.3}Universality}{29}{subsection.8.3}
\contentsline {paragraph}{Proof in dimension $p=1$.}{30}{section*.11}
\contentsline {paragraph}{Proof in arbitrary dimension $p$.}{30}{section*.12}
\contentsline {paragraph}{Quantitative rates.}{31}{section*.13}
\contentsline {section}{\numberline {9}Automatic Differentiation}{32}{section.9}
\contentsline {subsection}{\numberline {9.1}Finite Differences and Symbolic Calculus}{32}{subsection.9.1}
\contentsline {subsection}{\numberline {9.2}Computational Graphs}{32}{subsection.9.2}
\contentsline {subsection}{\numberline {9.3}Forward Mode of Automatic Differentiation}{33}{subsection.9.3}
\contentsline {paragraph}{Simple example.}{33}{section*.14}
\contentsline {paragraph}{Dual numbers.}{34}{section*.15}
\contentsline {subsection}{\numberline {9.4}Reverse Mode of Automatic Differentiation}{35}{subsection.9.4}
\contentsline {paragraph}{Back-propagation.}{35}{section*.16}
\contentsline {paragraph}{Simple example.}{36}{section*.17}
\contentsline {subsection}{\numberline {9.5}Feed-forward Compositions}{36}{subsection.9.5}
\contentsline {subsection}{\numberline {9.6}Feed-forward Architecture}{37}{subsection.9.6}
\contentsline {paragraph}{Multilayers perceptron.}{37}{section*.18}
\contentsline {paragraph}{Link with adjoint state method.}{38}{section*.19}
\contentsline {subsection}{\numberline {9.7}Recurrent Architectures}{38}{subsection.9.7}
\contentsline {paragraph}{Residual recurrent networks. }{39}{section*.20}
\contentsline {paragraph}{Mitigating memory requirement. }{39}{section*.21}
\contentsline {paragraph}{Fixed point maps}{40}{section*.22}
\contentsline {paragraph}{Argmin layers}{40}{section*.23}
\contentsline {paragraph}{Sinkhorn's algorithm}{40}{section*.24}
