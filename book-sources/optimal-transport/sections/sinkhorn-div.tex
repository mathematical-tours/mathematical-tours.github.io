% !TEX root = ../CourseOT.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sinkhorn Divergences}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dual of Sinkhorn}

%%%
\paragraph{Discrete dual.}

The following proposition details the dual problem associated to~\eqref{eq-regularized-discr}.

\begin{prop}
One has
\eql{\label{eq-dual-formulation}
	\MKD_\C^\epsilon(\a,\b) = \umax{\fD \in \RR^n,\gD \in \RR^m}
		 \dotp{\fD}{\a} + \dotp{\gD}{\b} 
		% - \epsilon\sum_{i,j} e^{ \frac{\fD_i+\gD_j-\C_{i,j}}{\epsilon} }.
%		- \epsilon \dotp{e^{\fD/\epsilon} }{ \K e^{\gD/\epsilon}}.
	- \epsilon \sum_{i,j} \exp\pa{
		\frac{\fD_i+\gD_j-\C_{i,j}}{\epsilon}
	} \a_i \b_j + \epsilon.
} 
%
The optimal $(\fD,\gD)$ are linked to scalings $(\uD,\vD)$ appearing in~\eqref{eq-scaling-form} through 
\eql{\label{eq-entropy-pd}
	(\uD,\vD)=(\a_i e^{\fD/\epsilon},\b_j e^{\gD/\epsilon}).
}
\end{prop}

\begin{proof}
We introduce Lagrange multiplier and consider
\eq{
	\umin{\P \geq 0} \umax{\fD,\gD} \dotp{\C}{\P} + \epsilon \KLD(\P|\a \otimes \b) + \dotp{\a-\P\ones}{\fD} + \dotp{\b-\P^\top\ones}{\gD}.
}
One can check that strong duality holds since the function is continuous and that one can exchange the min with the max to get
\eq{ 
	 \umax{\fD,\gD}  \dotp{\fD}{\a} + \dotp{\gD}{\b}
	 -	 
	 \epsilon \umin{\P \geq 0}
	 	\dotp{\frac{\fD\oplus\gD-\C}{\epsilon}}{\P} - \KLD(\P|\a \otimes \b)
		= 
		\dotp{\fD}{\a} + \dotp{\gD}{\b} - \epsilon \KLD^*\pa{ \frac{\fD\oplus\gD-\C}{\epsilon}|\a \otimes \b }.
}
One concludes by using~\eqref{eq-legendre} for $\phi(r)=r \log(r)-r+1$
\eq{
	\KLD^*(H|\a \otimes \b) = \sum_{i,j} \phi^*(e^{H_{i,j}}) \a_i \b_j.
}
and we have after simple computation that $\phi^*(s)=e^s-1$.
\end{proof}

%%%
\paragraph{Discrete soft $c$-transforms.}

Since the dual problem~\eqref{eq-dual-formulation} is smooth, one can consider an alternating minimization. For a fixed $\gD$, one can minimize with respect to $\fD$, which leads to the following equation to be solved when zeroing the derivative with respect to $\fD$ 
\eq{
	\a_i - e^{\frac{\fD_i}{\epsilon}} \a_i \sum_j \exp\pa{
		\frac{\gD_j-\C_{i,j}}{\epsilon}
	} \b_j = 0
}
which leads to the explicit solution 
\eq{
	\fD_i = -\epsilon \log \sum_j \exp\pa{ \frac{\gD_j-\C_{i,j}}{\epsilon} } \b_j.
}
We conveniently introduce the soft-min operator of some vector $h \in \RR^m$
\eq{
	{\min}_{\b}^\epsilon(h) \eqdef -\epsilon \log \sum_j e^{-h_j/\epsilon} \b_j
}
which is a smooth approximation of the minimum operator, and the optimal $\fD$ for a fixed $\gD$ is computed by a soft version of the $c$-transform
\eql{\label{eq-soft-c-1}
	\fD_i = {\min}_{\b}^\epsilon( \C_{i,\cdot} - \gD ).
} 
In a similar way, the optimal $\gD$ for a fixed $\fD$ is
\eql{\label{eq-soft-c-2}
	\gD_j = {\min}_{\a}^\epsilon( \C_{\cdot,j} - \fD ).
} 
Exponentiating these iterations, one retrieves exactly Sinkhorn algorithm. These iterations are however unstable for small $\epsilon$. To be able to apply the algorithm in this regime, one needs to stabilize it using the celebrated log-sum-exp trick. It follows from noticing that similarly to the minimum operator, one has 
\eq{
	{\min}_{\b}^\epsilon(h-\text{cst})={\min}_{\b}^\epsilon(h)-\text{cst}
}
and to replace the computation of ${\min}_{\b}^\epsilon(h)$ by its stabilized version (equal when using infinite precision computation) ${\min}_{\b}^\epsilon(h-\min(h)) +  \min(h)$. 

%%%
\paragraph{Continuous dual and soft-transforms.}

For generic (non-necessarily discrete) input measures $(\al,\be)$, the dual problem~\eqref{eq-dual-formulation} reads
\eql{\label{eq-dual-sinkh-cont}
	\usup{\f,\g \in \Cc(\X)\times\Cc(\Y)} \int_\X \f(x)\d\al(x) + \int_\Y \g(x)\d\be(x) 
		 - \epsilon \int_{\X\times\Y} \pa{e^{ \frac{f\oplus g-c}{\epsilon} }-1} \d\al \otimes \d\be
}
This corresponds to a smoothing of the constraint $\Potentials(\c)$ appearing in the original problem~\eqref{eq-dual-generic}, which is retrieved in the limit $\epsilon \rightarrow 0$.

The corresponding soft $c$-transform, which minimize this dual problem with respect to either $f$ or $g$ reads 
\begin{align*}
	\foralls y \in \Y, \quad
	\f^{\c,\epsilon}(y) &\eqdef - \epsilon \log\pa{ 
			\int_{\X} e^{\frac{-\c(x,y) + \f(x)}{\epsilon}} \d\al(x)
	},\\
	\foralls x \in \X, \quad
	\g^{\bar\c,\epsilon}(x) &\eqdef - \epsilon \log\pa{ 
			\int_{\Y} e^{\frac{-\c(x,y) + \g(y)}{\epsilon}} \d\be(y)
	}.
\end{align*}
In the case of discrete measures, on retrieves the formula~\eqref{eq-soft-c-1} and~\eqref{eq-soft-c-2}.

We omit the details, but similarly to the unregularized case, one can define an entropic semi-discrete problem and develop stochastic optimization method to solve it. 

The soft $c$-transform, beside being useful to define Sinkhorn's algorithm, is also important to show existence of dual solutions. 

\begin{prop}
	The dual problem~\eqref{eq-dual-sinkh-cont} has solutions, and the set of solutions is of the form 
	$(f^\star+\la,g^\star-\la)$ for $\la \in \RR$. 
\end{prop}
\begin{proof}
	There are several proof strategies showing the existence of solution, including (i) using the contractance of Sinkhorn for the Hilbert metric (see bellow), (ii) using the Lipschitz smoothness of the $c$-transforms which implies one can consider a compact set of continuous functions, (iii) using the fact that $f \mapsto f^{c,\epsilon}$ is a monotone map. 
	%
	Uniqueness follows from strict convexity of $H \mapsto \int e^{\frac{H}{\epsilon}} \d \a\otimes\be$ on the space of functions defined up to an additive constant, and that the kernel of the linear map $(f,g) \mapsto H = f \oplus g-c$ are constant functions of the form $f(x)=-g(x)=\la$. 
\end{proof}


%
%Proving existence (\emph{i.e.} the sup is actually a max) of these Kantorovich potentials $(\f,\g)$ in the case of entropic transport is less easy than for classical OT (because one cannot use $c$-transform and potentials are not automatically Lipschitz). Proof of existence can be done using the convergence of Sinkhorn iterations, see~\cite{2016-chizat-sinkhorn} for more details.

\begin{rem}[Convexity properties]
	The $\log \int \exp$ operator behaves similarly to the max operator, and in particular it preserves convexity.
	%
	Similarly to the unregularized case studied in Remark~\ref{rem-proof-brenier}, in the particular case $c(x,y)=-\dotp{x}{y}$, one thus has that $\f^{\c,\epsilon}$ is always a concave function, so that when $c(x,y)=\norm{x-y}^2/2$, one has that the optimal potentials are of the form $f^\star(x)=\norm{x}^2-\phi^\star(x)$ where $\phi^\star$ is a convex function.
\end{rem}


\begin{rem}[Gaussian marginals]
	Another remarkable properties of these soft $c$-transform in the case $c(x,y)=\norm{x-y}^2$ and $(\al,\be)$ are Gaussians is that it preserves quadratique functions, since product and convolution of Gaussian functions are Gaussians.
	%
	This implies that for Gaussian marginal, the optimal potentials $(f^\star,g^\star)$ are quadratic, and that the optimal entropic-regularized coupling $\pi^\star$ is a Gaussian.
\end{rem}
  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Convergence Analysis}
\label{sec-convergence-dual}


This section revisits the convergence analysis of Section~\ref{sec-convergence-init}

%%%
\paragraph{Bregman sub-linear convergence.}

One can show the following KL conservation during the iterations.

\begin{proposition}
	Denoting $(f_k,g_k)$ the dual variable generated by Sinkhorn's algorithm and $\pi_k = \exp(\frac{c-f \oplus g}{\epsilon}) \al \otimes \be$ the associated primal variable. The dual cost is $D(f,g)$ while the primal one is
	$\KL(\pi_{k+1}|\xi)$ where $\xi \eqdef e^{-c/\epsilon} \al \otimes \be$ is the Gibbs kernel. One has
	\eq{
		D(f_{k+1},g_{k+1}) - D(f_{k},g_{k})  = 
		\epsilon\KL(\pi^\star|\pi_{k}) - \epsilon\KL(\pi^\star|\pi_{k+1}) = 
		\epsilon\KL(\al|\pi_{k,1}) + \epsilon\KL(\be|\pi_{k,2})
		> 0. 
	}
\end{proposition}

\begin{proof}
	TODO. 
\end{proof}

One sees that for even index $k$, $\KL(\be|\pi_{k,2})=0$ while for even $k$, $\KL(\al|\pi_{k,1})=0$ (Sinkhorn corresponds to alternating projection on the constraints).

This lemma shows the strict decay of the dual and primal energies.
It implies that (assuming $D(f_0,g_0)=0$, using $f_0=g_0=0$)
\eq{
	\epsilon \sum_k \KL(\al|\pi_{k,1}) + \KL(\be|\pi_{k,2}) = D(f^\star,g^\star)
}
so that necessarily $\KL(\al|\pi_{k,1})$ and $\KL(\be|\pi_{k,2})$ are converging to zero. One cannot deduce from this a rate on the last iterate (since it is not clear $\KL(\al|\pi_1^k)$ is decaying), but at least 
\eq{
	\umin{\ell \leq k} \KL(\al|\pi_{k,1}) \leq \frac{D(f^\star,g^\star)}{\epsilon k} = \frac{\KL(\pi^\star|\xi)}{k}.
} 
where $\xi \eqdef \al\otimes\be e^{-\frac{c}{\epsilon}}$ is the Gibbs kernel.

%%%
\paragraph{Hilbert-metric linear convergence.}

The Hilbert metric convergence results is equivalent to convergence on the dual potentials $(f,g)$ according to the variational norms. For bounded cost $c$ (e.g. on compact spaces), 
\eq{
	\norm{f_k-f^\star}_V = O(\la^k)
	\qandq
	\norm{g_k-f^\star}_V = O(\la^k)
} 
\eq{
	\norm{ \log \frac{\d\pi_k}{\d\pi^\star} }_\infty
	=
	\norm{ (f_k-f^\star) \oplus (g_k-g^\star) }_\infty
	\leq 
	\norm{f_k-f^\star}_V + \norm{g_k-g^\star}_V
}
where the contraction ratio only depends on the radius $R \eqdef \sup_{x,y} c(x,y)$ of the cost, $\la=\frac{\sqrt{\eta}-1}{\sqrt{\eta}+1} \leq \text{tanh}(R/(2\epsilon)) < 1$. One also have the following bounds
\eq{
	\norm{f_k-f^\star}_V \leq \frac{ \norm{\log\frac{\d \pi_{k,1}}{\d\al}}_\infty }{1-\la}
}
which can be used to provide a posterior estimate on the rate of convergence and serves a stopping criterion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sinkhorn Divergences}
\label{sec-sinkhorn-div}


%%%
\paragraph{Entropic bias.}

A major issue of the value of Sinkhorn problem~\eqref{eq-entropic-generic} is that $\MK_\c^\epsilon(\al,\be)>0$. So in particular, 
\eq{
	\al_\epsilon = \uargmin{\be} \MK_\c^\epsilon(\al,\be)
}
does not satisfy $\al_\epsilon=\al$ unless $\epsilon=0$. The following proposition shows that the bias induced by this entropic regularization has a catastrophic influence in the large $\epsilon$ limit.

\begin{prop}
	One has $\MK_\c^\epsilon(\al,\be) \rightarrow \int c \d\al \otimes \be$ as $\epsilon \rightarrow +\infty$.
\end{prop}
\begin{proof}	
	The intuition of the proof follows from the fact that the optimal coupling converges to $\al \otimes \be$. 
\end{proof}

So in the large $\epsilon$ limit, $\MK_\c^\epsilon$ behaves like an inner product and not like a norm. For instance, in the case 
\eq{
	\al_\epsilon \rightarrow \umin{\be} \dotp{ \int c(x,\cdot) \d\al(x)}{\be} = \de_{y^\star(\al)}
	\qwhereq
	 y^\star(\al) = \uargmin{y} \int c(x,y) \d\al(x).
}
For instance, when $c(x,y)=\norm{x-y}^2$ then $\al_\epsilon$ collapses towards a Dirac located at the mean $\int x \d\al(x)$ of $\al$.

%%%
\paragraph{Sinkhorn divergences.}

The usual way to go from an inner product to a norm is to use the polarization formula, we thus also consider for the Sinkhorn cost, in order to define the debiased Sinkhorn divergence
\eq{
	\bar\MK_\c^\epsilon(\al,\be) \eqdef 
	\MK_\c^\epsilon(\al,\be) - \frac{1}{2} \MK_\c^\epsilon(\al,\al) - \frac{1}{2}\MK_\c^\epsilon(\al,\be).
}
It is not (yet) at all clear why this quantity should be positive. 

Before going on, we prove a fundamental lemma which states that the dual cost has a simple form where the regularization vanish at a solution (and actually it vanishes also during Sinkhorn's iteration by the same proof).

\begin{lem}
	Denoting $(f_{\al,\be},g_{\al,\be})$ optimal dual potentials (which can be shown to be unique up to an additive constant), one has
	\eql{\label{eq-formula-cost-dual}
		\MK_\c^\epsilon(\al,\be) = \dotp{f_{\al,\be}}{\al} + \dotp{g_{\al,\be}}{\be}.
	}
\end{lem}
\begin{proof}
	We first notice that at optimality, the relation 
	\eq{
		f_{\al,\be} = -\epsilon\log\int_\Yy e^{\frac{g_{\al,\be}(y)-c(x,y)}{\epsilon}} \d\be(y)
	}
	after taking the exponential, equivalently reads
	\eq{
		1 = \int_\Yy e^{\frac{f_{\al,\be}(x) + g_{\al,\be}(y)-c(x,y)}{\epsilon}} \d\be(y)
		\qarrq
		\int_{\Xx\times\Yy} \pa{ e^{\frac{f_{\al,\be} \oplus g_{\al,\be}-c}{\epsilon}} -1  } \d\al \otimes \be = 0.
	}
	Plugging this in formula~\eqref{eq-dual-sinkh-cont}, one obtains the result.
\end{proof}


%
Let us first show that its asymptotic makes sense.

\begin{prop}
	One has $\bar\MK_\c^\epsilon(\al,\be) \rightarrow \MK_\c(\al,\be)$ when $\epsilon\rightarrow 0$ and
	\eq{
		\bar\MK_\c^\epsilon(\al,\be) \rightarrow 
		\frac{1}{2}\int -c \d(\al-\be) \otimes \d(\al-\be)
		\qwhenq
		\epsilon \rightarrow +\infty.
	}	
\end{prop}
\begin{proof}For discrete measures, the convergence is already proved in Proposition~\eqref{eq-entropy-conv-2}, we now give a general treatment.
	\textbf{Case $\epsilon \rightarrow 0$.} 
	\todo{Do the proof of $\epsilon \rightarrow 0$ on non-discrete spaces}
	\textbf{Case $\epsilon \rightarrow +\infty$.} We denote $(f_\epsilon,g_\epsilon)$ optimal dual potential. Optimality condition on $f_\epsilon$ (equivalently Sinkhorn fixed point on $f_\epsilon$) reads
	\begin{align*}
		f_\epsilon &= -\epsilon \log \int \exp\pa{ \frac{g_\epsilon(y) - c(\cdot,y)}{\epsilon} } \d \be(y)
		=	-\epsilon \log \int \pa{ 1 + \frac{g_\epsilon(y) - c(\cdot,y)}{\epsilon} + o(1/\epsilon) } \d \be(y) \\		
		&=	-\epsilon \int \pa{ \frac{g_\epsilon(y) - c(\cdot,y)}{\epsilon} + o(1/\epsilon) } \d \be(y)
		= - \int g_\epsilon \d\be + \int c(\cdot,y) \d \be(y) + o(1).  
	\end{align*}
	Plugging this relation in the dual expression~\eqref{eq-formula-cost-dual}
	\eq{
		\MK_\c^\epsilon(\al,\be) = \int f_\epsilon \d\al + \int g_\epsilon \d \be = 
		- \iint c(x,y) \d\al(x)\d\be(y) + o(1). 
	}
\end{proof}

In the case where $-c$ defines a conditionnaly positive definite kernel, then $\bar\MK_\c^\epsilon(\al,\be)$ converges to the square of an Hilbertian kernel norm. A typical example is when $c(x,y)=\norm{x-y}^p$ for $0 < p < 2$, which corresponds to the so-called Energy distance kernel. This kernel norm is the dual of a homogeneous Sobolev norm.

We now show that this debiased Sinkhorn divergence is positive.

\begin{prop}
	If $k(x,y)=e^{-c(x,y)/\epsilon}$ is positive definite, then $\bar\MK_\c^\epsilon(\al,\be) \geq 0$ and is zero if and only if $\al=\be$.
\end{prop}

\begin{proof}	
	In the following, we denote $(f_{\al,\be},g_{\al,\be})$ optimal dual potential for the 
	dual Schrodinger problem between $\al$ and $\be$.
	We denote $f_{\al,\al}=g_{\al,\al}$ (one can assume they are equal by symmetry) the solution for the problem between $\al$ and itself. 
	%
	Using the suboptimal function $(f_{\al,\al},g_{\be,\be})$ in the dual maximization problem, and using relation~\eqref{eq-formula-cost-dual} for the simplified expression of the dual cost, on obtains
	\eq{
		\MK_\c^\epsilon(\al,\be) \geq \dotp{f_{\al,\al}}{\al} + \dotp{g_{\be,\be}}{\be}
			 - \epsilon \dotp{ e^{\frac{f_{\al,\be} \oplus g_{\al,\be}-c}{\epsilon}} -1 }{\al \otimes \be}
	}
	But one has $\dotp{f_{\al,\al}}{\al} = \frac{1}{2}\MK_\c^\epsilon(\al,\al)$ and same for $\be$, so that the previous inequality equivalently reads
	\eq{
		\frac{1}{\epsilon} \bar \MK_\c^\epsilon(\al,\be) 
			\geq 	1 - \dotp{ e^{\frac{f_{\al,\be} \oplus g_{\al,\be}-c}{\epsilon}} }{\al \otimes \be}
			= 1 - \dotp{\tilde\al}{\tilde\be}_{k}
	}
	where $\tilde\al=e^{f_{\al,\al}} \al$, $\tilde\be=e^{f_{\be,\be}} \be$
	and we introduced the inner product (which is a valid one because $k$ is positive) 
	$\dotp{\tilde\al}{\tilde\be}_{k} \eqdef \int k(x,y) \d\tilde\al(x) \d\tilde\be(y)$.
	One note that Sinkhorn fixed point equation, once exponentiated, reads
	$e^{f_{\al,\al}} \odot [ k( \tilde \al ) ] = 1$ and hence
	\eq{
		\norm{\tilde \al}_k^2 = \dotp{k(\tilde\al)}{\tilde\al}
		= \dotp{e^{f_{\al,\al}} \odot k(\tilde\al)}{\al} = \dotp{1}{\al}=1
	}
	and similarly $\norm{\tilde \be}_k^2=1$. So by Cauchy-Schwartz, one has
	$1 - \dotp{\tilde\al}{\tilde\be}_{k} \geq 0$.
	% 
	Showing strict positivity is more involved, and is not proved here. 
\end{proof}

One can furthermore show that this debiased divergence metrizes the convergence in law. 
      

   
   
